# Корреляционный анализ {#andan-cor}

{{< include other/_symbols.qmd >}}

```{r opts, echo=FALSE, eval=TRUE, message=FALSE, warning=FALSE, error=FALSE}
knitr::opts_chunk$set(echo=TRUE, eval=TRUE, message=FALSE, warning=FALSE, error=FALSE)
```

```{r pkgs, echo=FALSE}
library(tidyverse)
theme_set(theme_bw())
theme_update(legend.position = "bottom")
```

:::{.intro}
При изучении [описательной статистики](part-desc.qmd) мы рассматривали отдельные переменные и описывали их поведение на нашей выборке. Однако в ситуации реального исследования нас интересуют взаимосвязи между различными переменными --- это то, ради чего мы затеваем наше исследование. Мы хотим узнать, как различные переменные связаны друг с другом, и ровно это мы и закладываем в наши гипотезы. Что ж, давайте начнём знакомиться с инструментами, которые позволят получить ответы на подобные вопросы.

Не будет преувеличением сказать, что корреляция есть база и основа всех методов анализа данных. Именно поэтому мы начинаем с неё. Итак, в путь!
:::


## Совместная изменчивость признаков {#andan-cor-covariance}

:::{.lab-junior}
:::

Пусть у нас есть две непрерывные переменные (случайные величины) $X$ и $Y$. Мы можем визуализировать их взаимосвязь, расположив имеющиеся у нас наблюдения в осях $X$ и $Y$. Иначе говоря, мы можем построить следующую визуализацию (@fig-cor-scheme-pos-start).

![Схема визуализации взаимосвязи между двумя непрерывными переменными](img/andan-cor/cor-scheme-pos-start.jpg){#fig-cor-scheme-pos-start}

Связь между переменными отчетливо опознается --- с ростом значений одной переменной растут значения другой переменной. Мы даже можем примерно провести некоторую прямую, описывающую такую взаимосвязь (@fig-cor-scheme-pos-trend).

![Связь легко схватывается визуально](img/andan-cor/cor-scheme-pos-trend.jpg){#fig-cor-scheme-pos-trend}

Но пока оставим прямую в покое. Как бы нам описать имеющуюся ситуацию математически? Ну, пожалуй, не будет большим открытием, если я скажу, что мы можем посчитать средние [арифметические] по обеим переменным. Действительно, среднее --- это одна из описательных статистик, и имея ряд наблюдений мы запросто сможем его посчитать. Добавим на нашу визуализацию средние по обеим переменным (@fig-cor-scheme-pos-means).

![Средние по обеим переменным на визуализации](img/andan-cor/cor-scheme-pos-means.jpg){#fig-cor-scheme-pos-means}

Чем нам это поможет? Средние разбили наши наблюдения на четыре части. То, в какой из частей будет находится конкретное наблюдение, описывается его отклонением по обеим переменным (@fig-cor-scheme-pos-deviations). Заметим, что у нас получается:

* много *сонаправленных отклонений* --- таких наблюдений, для которых $(x - \overline X) > 0 \wedge (y - \overline Y) > 0$ или $(x - \overline X) < 0 \wedge (y - \overline Y) < 0$;.
* мало *разнонаправленных отклонений* --- таких наблюдений, для которых $(x - \overline X) < 0 \wedge (y - \overline Y) > 0$ или $(x - \overline X) < 0 \wedge (y - \overline Y) > 0$.

![Отклонения от средних по обеим переменным](img/andan-cor/cor-scheme-pos-deviations.jpg){#fig-cor-scheme-pos-deviations}

Как это можно использовать для описания закономерности? Давайте обратим внимание на знак произведения отклонений в каждой из частей наблюдений (@fig-cor-scheme-pos-signs):

* если отклонения *сонаправлены*, то их произведение *положительно*:

$$
\begin{split}
\overset{\circled{+}}{(x - \overline X)} \overset{\circled{+}}{(y - \overline Y)} &> 0 \\
\overset{\circled{-}}{(x - \overline X)} \overset{\circled{-}}{(y - \overline Y)} &> 0
\end{split}
$${#eq-deviation-same-direct}

* если отклонения *разнонаправлены*, то их произведение *отрицательно*:

$$
\begin{split}
\overset{\circled{+}}{(x - \overline X)} \overset{\circled{-}}{(y - \overline Y)} &< 0 \\
\overset{\circled{-}}{(x - \overline X)} \overset{\circled{+}}{(y - \overline Y)} &< 0
\end{split}
$${#eq-deviation-diff-direct}

![Знаки произведений отклонений по обеим переменным](img/andan-cor/cor-scheme-pos-signs.jpg){#fig-cor-scheme-pos-signs}

Отлично! Но нас мало интересуют отклонения для конкретных наблюдений --- нам хочется узнать, как дело обстоит в целом, то есть какова (со)направленность отклонений *в среднем*. Со средним мы знакомы, давайте его посчитаем:

$$
\frac{1}{n} \sum_{i=1}^n (x - \overline X)(y - \overline Y) > 0,
$${#eq-mean-codeviation-gzero}

где $n$ --- количество наблюдений (для обеих переменных оно одинаково).

По сути @eq-mean-codeviation-gzero описывает среднее отклонение наблюдений по двум переменным. Так как сонаправленных отклонений у нас больше, то среднее будет положительно.


Хорошо. А будет ли это работать для случая, когда связь между переменными будет *обратной*? Попробуем изобразить такой случай (@fig-cor-scheme-neg-start).

![Схема визуализации обратной взаимосвязи между двумя непрерывными переменными](img/andan-cor/cor-scheme-neg-start.jpg){#fig-cor-scheme-neg-start}

Вновь связь между переменными отчетливо опознается --- с ростом значений одной переменной значения другой переменной уменьшаются. И мы вновь можем примерно провести некоторую прямую, описывающую такую взаимосвязь (@fig-cor-scheme-neg-trend).

![Обратная связь также легко схватывается визуально](img/andan-cor/cor-scheme-neg-trend.jpg){#fig-cor-scheme-neg-trend}

Опять же, оставим пока прямую, и добавим средние по нашим переменным на визуализацию (@fig-cor-scheme-neg-means). Они, безусловно, как и в предыдущем случае разбьют наши наблюдения на четыре части.

![Средние по обеим переменным на визуализации обратной связи](img/andan-cor/cor-scheme-neg-means.jpg){#fig-cor-scheme-neg-means}

Однако в этом случае у нас получится, что (@fig-cor-scheme-neg-deviations):

* много *разнонаправленных отклонений* --- таких наблюдений, для которых $(x - \overline X) < 0 \wedge (y - \overline Y) > 0$ или $(x - \overline X) < 0 \wedge (y - \overline Y) > 0$.
* мало *сонаправленных отклонений* --- таких наблюдений, для которых $(x - \overline X) > 0 \wedge (y - \overline Y) > 0$ или $(x - \overline X) < 0 \wedge (y - \overline Y) < 0$;.

![Отклонения от средних по обеим переменным при обратной связи ](img/andan-cor/cor-scheme-neg-deviations.jpg){#fig-cor-scheme-neg-deviations}

Имея в виду знаки произведений отклонений ([-@eq-deviation-same-direct] и [-@eq-deviation-diff-direct], @fig-cor-scheme-neg-signs), можем заметить, что в этом случае среднее отклонение наблюдений по двум переменным будет отрицательным:

$$
\frac{1}{n} \sum_{i=1}^n (x - \overline X)(y - \overline Y) < 0,
$${#eq-mean-codeviation-lzero}

![Знаки произведений отклонений по обеим переменным при обратной связи](img/andan-cor/cor-scheme-neg-signs.jpg){#fig-cor-scheme-neg-signs}

:::{.callout-note appearance="minimal"}
Мы получили некое выражение ([-@eq-mean-codeviation-gzero] и [-@eq-mean-codeviation-lzero]), по знаку которого мы можем судить о направлении связи между переменными --- прямая ([-@fig-cor-scheme-pos-start]) или обратная ([-@fig-cor-scheme-neg-start]).
:::

:::{#exr-cov-0}
Мы рассмотрели выше случаи, когда связь между переменными (случайными величинами) есть. Однако вполне возможна ситуация, когда связь между переменными отсутствует.

Постройте схематичную визуализацию для такого случая и на основе неё выясните, что нам скажет найденное нами выражение.
:::

::::{.solution}
:::{.cell}

Ситуацию отсутствия взаимосвязи между двумя переменными можно представить так:

![Схема визуализации отсутствия взаимосвязи между двумя непрерывными переменными](img/andan-cor/cor-scheme-zero-start.jpg)

Добавив знаки отклонений от средних значений, увидим, что сонаправленных и разнонаправленных отклонений оказывается примерно поровну.

![Знаки произведений отклонений по обеим переменным при отсутствии связи](img/andan-cor/cor-scheme-zero-signs.jpg)

Следовательно, в этом случае

$$
\frac{1}{n} \sum_{i=1}^n (x - \overline X)(y - \overline Y) \approx 0
$$

:::
::::



## Ковариация {#andan-cor-cov}

:::{.lab-junior}
:::

По сути, в предыдущем разделе мы изучали *ковариацию*.

:::{#def-cov}
**Ковариация** --- мера совместной изменчивости двух переменных (случайных величин).
:::

Само название **ковариация (covariance)** обозначает **совместную** (**ко-**) **изменчивость** (**-вариацию**)[^co-variance].

[^co-variance]: А *вариация* (*variance*) --- это другое название дисперсии, как мы обсуждали ранее.

Более того, мы практически получили формулу расчета ковариации двух переменных --- @eq-mean-codeviation-gzero (и [-@eq-mean-codeviation-lzero]). Однако полученные нами формулы будут давать смещённую оценку ковариации. Причина абсолютно аналогична той, что была в дисперсии --- мы используем выборочные средние, чтобы рассчитать отклонения. Для того, чтобы получить несмещенную оценку ковариации, необходимо скорректировать формулу --- делить не на $n$ (количество наблюдений), а на $n-1$. Тогда получим, что ковариацию двух переменных можно вычислить по имеющимся наблюдениям по формуле:

$$
\cov(X, Y) = \frac{1}{n-1} \sum_{i=1}^n (x_i - \overline X) (y_i - \overline Y),
$${#eq-cov-sample}

где $\cov(X, Y)$ --- ковариация переменных $X$ и $Y$, $n$ --- количество наблюдений, $\overline X$ и $\overline Y$ --- выборочные средние.

:::{#exr-cov-xx}
Чему будет равна ковариация переменной с самой собой, то есть $\cov(X, X)$?
:::

::::{.solution}
:::{.cell}
$$
\begin{split}
\cov(X,X) &= \frac{1}{n-1}\sum_{i=1}^n (x_i - \overline X)(x_i - \overline X) = \\
&= \frac{1}{n-1} \sum_{i=1}^n (x_i - \overline X)^2 \defin{=} \var X
\end{split}
$$
:::
::::


### Интерпретация значения ковариации {#andan-cor-cov-interpret}

:::{.lab-junior}
:::

Наша поправка в формуле [-@eq-cov-sample] не повлияет на наблюдения, произведенные выше. Мы отметили, что

:::{.callout-important appearance="minimal"}

* в случае **прямой взаимосвязи** между переменными **ковариация положительна**,
* в случае **обратной взаимосвязи** между переменными **ковариация отрицательна**,
* в случае **отсутствия взаимосвязи** между переменными **ковариация равна нулю** (см. @exr-cov-0).

:::

Таким образом, теперь на основе ковариации мы можем определять **направление** связи.

Можно ли что-либо сказать о *силе связи* на основе ковариации? Тут хорошо бы сначала определиться с тем, что вообще такое сила связи. Не совсем понятно, как это определить строго, но давайте посмотрим на следующие картинки (@fig-cov-var).

```{r cov-var-data}
#| code-fold: true
#| code-summary: "Код создания симуляции"

n <- 100
set.seed(616)
tibble(
  x1 = rnorm(n, sd = 1),
  y1 = 0.4 * x1 + rnorm(n, mean = 2, sd = 1 - 0.4^2),
  x2 = rnorm(n, sd = 2),
  y2 = 0.4 * x2 + rnorm(n, mean = 2, sd = 2 - 2 * 0.4^2),
  x3 = rnorm(n, sd = 1),
  y3 = -0.8 * x3 + rnorm(n, mean = 2, sd = 1 - 0.8^2),
  x4 = rnorm(n, sd = 4),
  y4 = -0.8 * x4 + rnorm(n, mean = 4, sd = 4 - 4 * 0.8^2)
) -> sim_cov


sim_cov %>% 
  ggplot(aes(x1, y1)) +
  geom_point() +
  geom_smooth(method = "lm", 
              se = FALSE,
              color = "gray60") +
  labs(x = "X", y = "Y") -> cov1

sim_cov %>% 
  ggplot(aes(x2, y2)) +
  geom_point() +
  geom_smooth(method = "lm", 
              se = FALSE,
              color = "gray60") +
  labs(x = "X", y = "Y") -> cov2

sim_cov %>% 
  ggplot(aes(x3, y3)) +
  geom_point() +
  geom_smooth(method = "lm", 
              se = FALSE,
              color = "gray60") +
  labs(x = "X", y = "Y") -> cov3

sim_cov %>% 
  ggplot(aes(x4, y4)) +
  geom_point() +
  geom_smooth(method = "lm",
              se = FALSE,
              color = "gray60") +
  labs(x = "X", y = "Y") -> cov4
```

```{r cov-var-plots, echo=FALSE}
#| code-fold: true
#| code-summary: "Код построения визуализации"
#| label: fig-cov-var
#| fig-cap: "Визуализация симуляций при разных параметрах распределений случайных величин $X$ и $Y$ ($n = 100$)"
#| fig-subcap:
#|   - "$X_1 \\thicksim \\norm(0, 1)$, $\\,Y_1 \\thicksim \\norm(2, 0.95)$, $\\cov(X_1, Y_1) = 0.23$"
#|   - "$X_2 \\thicksim \\norm(0, 4)$, $\\,Y_2 \\thicksim \\norm(2, 3.30)$, $\\cov(X_2, Y_2) = 1.02$"
#|   - "$X_3 \\thicksim \\norm(0, 1)$, $\\,Y_3 \\thicksim \\norm(2, 0.80)$, $\\cov(X_3, Y_3) = -0.81$"
#|   - "$X_4 \\thicksim \\norm(0, 16)$, $\\,Y_4 \\thicksim \\norm(4, 10.20)$, $\\cov(X_4, Y_4) = -10.71$"
#| layout-ncol: 2

print(cov1)
print(cov2)
print(cov3)
print(cov4)
```

Видно, что связи между $X$ и $Y$ на первых двух визуализациях ([-@fig-cov-var-1] и [-@fig-cov-var-2]) прямые, а на двух вторых ([-@fig-cov-var-3] и [-@fig-cov-var-4]) --- обратные. При этом также визуально мы можем сказать, что обратные связи *сильнее*, чем прямые --- наблюдения активнее группируются вокруг прямой, описывающей закономерность. Однако если сравнивать между собой обе прямые или обе обратные связи, то по силе они приблизительно равны.

Попробуем посчитать ковариацию между переменными в четырех имеющихся ситуациях:

```{r cov-var-covs}
#| code-fold: true
#| code-summary: "Код расчета ковариаций"
#| label: tbl-cov-var-values
#| tbl-cap: Значения ковариаций в симуляции

sim_cov %>% 
  summarise(
    `$$\\text{cov}(X_1, Y_1)$$` = cov(x1, y1) %>% round(2) %>% paste0("$$", ., "$$"),
    `$$\\text{cov}(X_2, Y_2)$$` = cov(x2, y2) %>% round(2) %>% paste0("$$", ., "$$"),
    `$$\\text{cov}(X_3, Y_3)$$` = cov(x3, y3) %>% round(2) %>% paste0("$$", ., "$$"),
    `$$\\text{cov}(X_4, Y_4)$$` = cov(x4, y4) %>% round(2) %>% paste0("$$", ., "$$")
  ) %>% 
  knitr::kable(format = "html",
               align = "c")
```

Наблюдаем нечто интересное: визуально связи между $X_1$ и $Y_1$ ([-@fig-cov-var-1]) и между $X_2$ и $Y_2$ ([-@fig-cov-var-2]) примерно равны по силе, однако значения ковариаций различаются: $\cov(X_1, Y_1) =$ `r round(cov(sim_cov$x1, sim_cov$y1), 2) %>% str_replace("-", "−")`, $\cov(X_2, Y_2) =$ `r round(cov(sim_cov$x2, sim_cov$y2), 2) %>% str_replace("-", "−")`. Конечно, значения различаются не сильно --- может быть это не так существенно? Посмотрим на обратные связи. Визуально связи между $X_3$ и $Y_3$ ([-@fig-cov-var-3]) и между $X_4$ и $Y_4$ ([-@fig-cov-var-4]) также примерно равны по силе, однако значения ковариаций существенно различаются --- на целый порядок: $\cov(X_3, Y_3) =$ `r round(cov(sim_cov$x3, sim_cov$y3), 2) %>% str_replace("-", "−")`, $\cov(X_4, Y_4) =$ `r round(cov(sim_cov$x4, sim_cov$y4), 2) %>% str_replace("-", "−")`.

Почему же ковариация так сильно различается? Всё дело в дисперсии. Давайте рассчитаем дисперсию переменных и сопоставим её с ковариацией (@tbl-cov-var-vars). Мы видим, что в случаях, где дисперсия переменных выше, ковариация оказывается также выше.

```{r cov-var-vars}
#| code-fold: true
#| code-summary: "Код расчета вариаций"
#| label: tbl-cov-var-vars
#| tbl-cap: Значения выборочных вариаций (дисперсий) и ковариаций в симуляции

sim_cov %>% 
  summarise(
    # `$$\\overline X$$__$$a$$` = mean(x1),
    # `$$\\overline Y$$__$$a$$` = mean(y1),
    # `$$\\overline X$$__$$b$$` = mean(x2),
    # `$$\\overline Y$$__$$b$$` = mean(y2),
    # `$$\\overline X$$__$$c$$` = mean(x3),
    # `$$\\overline Y$$__$$c$$` = mean(y3),
    # `$$\\overline X$$__$$d$$` = mean(x4),
    # `$$\\overline Y$$__$$d$$` = mean(y4),
    `$$\\text{var} \\,X$$__a` = var(x1) %>% round(2) %>% paste0("$$", ., "$$"),
    `$$\\text{var} \\,Y$$__a` = var(y1) %>% round(2) %>% paste0("$$", ., "$$"),
    `$$\\text{var} \\,X$$__b` = var(x2) %>% round(2) %>% paste0("$$", ., "$$"),
    `$$\\text{var} \\,Y$$__b` = var(y2) %>% round(2) %>% paste0("$$", ., "$$"),
    `$$\\text{var} \\,X$$__c` = var(x3) %>% round(2) %>% paste0("$$", ., "$$"),
    `$$\\text{var} \\,Y$$__c` = var(y3) %>% round(2) %>% paste0("$$", ., "$$"),
    `$$\\text{var} \\,X$$__d` = var(x4) %>% round(2) %>% paste0("$$", ., "$$"),
    `$$\\text{var} \\,Y$$__d` = var(y4) %>% round(2) %>% paste0("$$", ., "$$"),
    `$$\\text{cov} (X,Y)$$__a` = cov(x1, y1) %>% round(2) %>% paste0("$$", ., "$$"),
    `$$\\text{cov} (X,Y)$$__b` = cov(x2, y2) %>% round(2) %>% paste0("$$", ., "$$"),
    `$$\\text{cov} (X,Y)$$__c` = cov(x3, y3) %>% round(2) %>% paste0("$$", ., "$$"),
    `$$\\text{cov} (X,Y)$$__d` = cov(x4, y4) %>% round(2) %>% paste0("$$", ., "$$")
  ) %>% 
  pivot_longer(cols = everything()) %>% 
  separate(name, into = c("stat", "Визуализация"), sep = "__") %>% 
  pivot_wider(names_from = stat, values_from = value) %>% 
  knitr::kable(format = "html",
               align = "c") %>% 
  kableExtra::column_spec(1, extra_css = 'vertical-align: middle !important;') %>% 
  kableExtra::row_spec(0, extra_css = 'vertical-align: middle !important;')
```

Итак, мы пронаблюдали, что

:::{.callout-important appearance="minimal"}
по значению ковариации мы не можем судить о силе связи, так как оно зависит от дисперсии переменных
:::

И всё же сила связи нам в исследованиях была бы очень интересна --- нам необходима какая-то новая статистика, лишенная этого недостатка ковариации.



## Корреляция {#andan-cor-correlation}

:::{.lab-junior}
:::

Перед нами стоит задача избавиться от влияния дисперсии на значение ковариации. Но избавляться от дисперсии мы уже умеем --- в этом нам поможет **стандартизация**.

Мы можем перед расчетом ковариации между переменными $X$ и $Y$ стандартизировать эти переменные. Получим новые переменные $X^\star$ и $Y^\star$, которые будут определяться как

$$
\begin{split}
X^\star &= \frac{X - \overline X}{\sd_X} \\
Y^\star &= \frac{Y - \overline Y}{\sd_Y}
\end{split}
$$

Дисперсия этих переменных будет равна единице, а попутно мы ещё получим среднее, равное нулю. Теперь мы можем рассчитать ковариацию между ними --- получим **корреляцию** между переменными $X$ и $Y$:

$$
\begin{split}
\cov(X^\star, Y^\star) &= \frac{1}{n-1} \sum_{i=1}^n (x^\star_i - \overline X^\star)(y^\star_i - \overline Y^\star) =\\
&= \frac{1}{n-1} \sum_{i=1}^n x^\star_i y^\star_i \defin{=} \cor(X, Y)
\end{split}
$${#eq-cor-stand-vars}

:::{#def-cor-stand-vars}
**Коэффициентом корреляции** называется ковариация стандартизированных случайных величин.
:::

Мы можем не стандартизировать переменные изначально, а выполнить преобразования по ходу расчета корреляции. Тогда получим, что

$$
\begin{split}
\cor (X, Y) &\defin{=} \cov(X^\star, Y^\star) = \\
&= \frac{1}{n-1} \sum_{i=1}^n \frac{x_i - \overline X}{\sd_X} \cdot \frac{y_i - \overline Y}{\sd_Y} = \\
&= \frac{1}{n-1} \sum_{i=1}^n \frac{1}{\sd_X \, \sd_Y} (x_i - \overline X)(y_i - \overline Y) = \\
&= \frac{1}{\sd_X \, \sd_Y} \lp \frac{1}{n-1} \sum_{i=1}^n (x_i - \overline X) (y_i - \overline Y) \rp = \\
&= \frac{\cov(X,Y)}{\sd_X \, \sd_Y}
\end{split}
$${#eq-cor-stand-cov}

:::{#def-cor-stand-cov}
**Коэффициентом корреляции** называется стандартизированное значение ковариации.
:::

Стандартизация случайных величин --- или же стандартизация значения ковариации --- приводит к следующим свойствам коэффициента корреляции:

:::{.callout-important appearance="minimal"}

* Знак коэффициента корреляции совпадает со знаком ковариации:
  * при **прямой взаимосвязи** коэффициент корреляции **положителен**,
  * при **обратной** --- **отрицателен**,
  * при **отсутствии взаимосвязи** --- **равен нулю**.

$$
\sgn \cor(X, Y) = \sgn \cov(X, Y)
$$

* Значение коэффициента корреляции может изменяться от $-1$ до $1$.

$$
\cor(X, Y) \in [-1, 1]
$$

:::



### Интерпретация значения коэффициента корреляции {#andan-cor-cor-interpret}

:::{.lab-junior}
:::

Итак, мы выяснили, как вычисляется коэффициент корреляции. Что мы можем сказать по его значению?

Прежде всего, давайте отметим следующее:

:::{#def-correlation}
**Корреляция** --- мера *линейной* взаимосвязи двух переменных (случайных величин).
:::

Теперь разберемся с этим подробнее.


#### Сила и направление взаимосвязи {#andan-cor-cor-interpret-force-direction}

:::{.lab-junior}
:::

В отличие от ковариации, коэффициент корреляции показывается не только **направление** взаимосвязи между переменными, но и её **силу**. Как отмечалось несколькими строками выше,

:::{.callout-important appearance="minimal"}

**Знак коэффициента корреляции** соответствует **направлению** взаимосвязи между переменными:

* при **прямой взаимосвязи** коэффициент корреляции **положителен**,
* при **обратной** --- **отрицателен**,
* при **отсутствии взаимосвязи** --- **равен нулю**.

:::

Силе связи между переменными будет соответствовать абсолютное значение корреляции, то есть $|r|$. Совершенно однозначно можно определить три значения и соответствующую им интерпретацию:

:::{.callout-important appearance="minimal"}

* $|r| = 1$ --- полная линейная взаимосвязь между переменными
  * $r = 1$ --- полная прямая линейная взаимосвязь
  * $r = -1$ --- полная обратная линейная взаимосвязь
* $r = 0$ --- отсутствие линейной взаимосвязи

:::

С другими значениями возникают сложности. Базово, конечно, можно разделить диапазон на какие-либо равные части и сопоставить им некоторую интерпретацию. Один из вариантов представлен в таблице [-@tbl-cor-interpret-basic].

:::{#tbl-cor-interpret-basic}

| Абсолютное значение коэффициента $|r|$ | Интерпретация силы взаимосвязи |
|:---:|:---:|
| $1.0$ --- $0.8$ | очень сильная |
| $0.8$ --- $0.6$ | сильная |
| $0.6$ --- $0.4$ | средняя |
| $0.4$ --- $0.2$ | слабая |
| $0.2$ --- $0.0$ | очень слабая |

Один из базовых вариантов интерпретации значения коэффициента корреляции как меры силы взаимосвязи между переменными
:::

```{r cor-linear-data, cache=TRUE}
#| code-fold: true
#| code-summary: "Код создания симуляции"

set.seed(2525)
n <- 500
cors <- seq(-1, 1, by = .1)
cors_tibble <- tibble(x = rnorm(n))

for (cor in cors) {
  cors_tibble[[paste0(cor, "_1")]] <- cor * cors_tibble$x + rnorm(n, sd = sqrt(1 - cor^2))
  cors_tibble[[paste0(cor, "_2")]] <- cor * cors_tibble$x
  cors_tibble[[paste0(cor, "_3")]] <- 0.5 * cors_tibble$x + rnorm(n, sd = sqrt(1 - cor^2))
}
```

```{r cor-force-plot}
#| code-fold: true
#| code-summary: "Код построения визуализации"
#| label: fig-cor-force
#| fig-cap: "Визуализация силы взаимосвязи переменных $X$ и $Y$ и соответствующих значений коэффициента корреляции ($n = 500$)"

cors_tibble %>% 
  pivot_longer(cols = -x) %>% 
  filter(str_detect(name, "_1$")) %>% 
  mutate(name = str_remove(name, "_1$") %>% 
           str_replace("-", "−") %>% 
           factor(
             levels = rep(seq(-1, 1, by = .1)) %>% str_replace("-", "−")
           )
         ) %>% 
  ggplot(aes(x, value)) +
  geom_point(size = 1,
             shape = 21,
             fill = "black",
             alpha = .5) +
  geom_smooth(method = "lm", 
              se = FALSE, 
              color = "red",
              linewidth = 1) +
  facet_wrap( ~ name, ncol = 7) +
  coord_fixed(ratio = 1) +
  labs(x = "X", y = "Y") +
  theme(strip.text = element_text(face = "bold")) -> cor_force_plot

print(cor_force_plot)
```

Такой подход имеет право на существование, и вполне интуитивен, если присмотреться к визуализации различных значений корреляции (@fig-cor-force). Однако необходимо помнить две важные вещи:

* разные области науки --- как и разные области психологии --- используют **разные измерительные инструменты**
  * в измерениях содержится *различная доля ошибки*
* разные области науки --- как и, опять же, разные области психологии --- изучают **различные феномены**
  * сила взаимосвязей, которые мы *можем ожидать*, будет разной

ПРИМЕР

В силу такого положения дел для разных областей науки существуют свои гайдлайны относительно пороговых значений для интерпретации коэффициента корреляции.



#### Наклон линии тренда {#andan-cor-cor-interpret-slope}

:::{.lab-junior}
:::

Рассматривая визуализацию [-@fig-cor-force] хочется заключить, что корреляция определяет угол наклона линии тренда. Такое желание интуитивно оправдано, однако **подобное заключение будет неверным**.

Действительно, коэффициент корреляции [связан](andan-simplelinear.qmd#andan-simplelinear-solve-cor) с углом наклона линии тренда, однако **не определяет его напрямую**. Как именно устроена эта связь, мы узнаем чуть позже, пока же приведем два контрпримера:

* при одинаковом значении корреляции линия тренда может быть расположена по-разному (@fig-cor-slope-1)
* при одинаковом положении линии тренда значения корреляции могут быть различны (@fig-cor-slope-2)

```{r cor-slope-plot}
#| code-fold: true
#| code-summary: "Код построения визуализации"
#| label: fig-cor-slope
#| fig-cap: "Корреляция и наклон линии тренда ($n = 500$)"
#| fig-subcap:
#|   - "Одинаковая корреляции при разном наклоне линии тренда"
#|   - "Разная корреляция при одинаковом наклоне линии тренда"

cors_tibble %>% 
  pivot_longer(cols = -x) %>% 
  filter(str_detect(name, "_2$")) %>% 
  mutate(name = str_remove(name, "_2$") %>% 
           factor(
             levels = rep(seq(-1, 1, by = .1))
           )
  ) %>% 
  ggplot(aes(x, value)) +
  geom_point(size = 1,
             shape = 21,
             fill = "black",
             alpha = .5) +
  geom_smooth(method = "lm", 
              se = FALSE, 
              color = "red",
              linewidth = 1) +
  facet_wrap( ~ name, ncol = 7,
              labeller = labeller(
                name = c(
                  `-1` = "−1",
                  `-0.9` = "−1",
                  `-0.8` = "−1",
                  `-0.7` = "−1",
                  `-0.6` = "−1",
                  `-0.5` = "−1",
                  `-0.4` = "−1",
                  `-0.3` = "−1",
                  `-0.2` = "−1",
                  `-0.1` = "−1",
                  `0` = "NA",
                  `0.1` = "1",
                  `0.2` = "1",
                  `0.3` = "1",
                  `0.4` = "1",
                  `0.5` = "1",
                  `0.6` = "1",
                  `0.7` = "1",
                  `0.8` = "1",
                  `0.9` = "1",
                  `1` = "1"
                )
              )) +
  coord_fixed(ratio = 1) +
  labs(x = "X", y = "Y") +
  theme(strip.text = element_text(face = "bold")) -> cor_slope1_plot

cors_tibble %>% 
  pivot_longer(cols = -x) %>% 
  filter(str_detect(name, "_3$")) %>% 
  mutate(name = str_remove(name, "_3$") %>% 
           as.numeric() %>% 
           factor(
             levels = rep(seq(-1, 1, by = .1))
           )
  ) %>% 
  filter(name %in% seq(.4, 1, by = .1)) %>% 
  summarise(cor = cor(x, value),
            .by = name) -> cor_same_slope
lbls <- cor_same_slope$cor
names(lbls) <- cor_same_slope$name

cors_tibble %>% 
  pivot_longer(cols = -x) %>% 
  filter(str_detect(name, "_3$")) %>% 
  mutate(name = str_remove(name, "_3$") %>% 
           as.numeric() %>% 
           factor(
             levels = rep(seq(-1, 1, by = .1))
           )
  ) %>% 
  filter(name %in% c(.4, .5, .7, .8, .9, 1)) %>% 
  ggplot(aes(x, value)) +
  geom_point(size = 1,
             shape = 21,
             fill = "black",
             alpha = .5) +
  geom_smooth(method = "lm", 
              se = FALSE, 
              color = "red",
              linewidth = 1) +
  facet_wrap( ~ name, ncol = 3,
              labeller = labeller(
                name = c(
                  `0.4` = "0.47",
                  `0.5` = "0.55",
                  `0.7` = "0.57",
                  `0.8` = "0.63",
                  `0.9` = "0.74",
                  `1` = "1.00"
                  )
                )
              ) +
  coord_fixed(ratio = 1) +
  labs(x = "X", y = "Y") +
  theme(strip.text = element_text(face = "bold")) -> cor_slope2_plot

print(cor_slope1_plot)
print(cor_slope2_plot)
```


:::{#exr-cor-nan}
В самом центре визуализации [-@fig-cor-slope-1] расположена диаграмма рассеяния, для которой значение коэффициента корреляции обозначено как `NA` (Not Available). Иначе говоря, в этом случае значение коэффициента корреляции не определено. Однако если посмотреть на картинку [-@fig-cor-force], то там на таком же месте визуализируется корреляция, равная нулю.

Для удобства оба графика представлены ниже. Почему в случае [-@fig-cor-nan-plot-2] корреляция не определена, хотя положение линии тренда такое же, как и в случае [-@fig-cor-nan-plot-1]?

```{r exr-cor-nan-plot, echo=FALSE}
#| label: fig-cor-nan-plot
#| fig-cap: "Два похожих, но различных случая ($n = 500$)"
#| fig-subcap:
#|   - "Коэффициент корреляции равен нулю"
#|   - "Коэффициент корреляции не определён"
#| layout-ncol: 2

set.seed(2525)
tibble(
  x = rnorm(500),
  y1 = rnorm(500),
  y2 = 0
) -> cor_nan

cor_nan %>% 
  ggplot(aes(x, y1)) +
  geom_point(size = 3,
             shape = 21,
             fill = "black",
             alpha = .5) +
  geom_smooth(method = "lm", 
              se = FALSE, 
              color = "red",
              linewidth = 2) +
  coord_fixed(ratio = 1) +
  facet_wrap(~ "0") +
  labs(x = "X", y = "Y") +
  theme(strip.text = element_text(face = "bold",
                                  size = 18)) -> cor_nan_1

cor_nan %>% 
  ggplot(aes(x, y2)) +
  geom_point(size = 3,
             shape = 21,
             fill = "black",
             alpha = .5) +
  geom_smooth(method = "lm", 
              se = FALSE, 
              color = "red",
              linewidth = 2) +
  coord_fixed(ratio = 1) +
  ylim(-3, 3) +
  facet_wrap(~ "NA") +
  labs(x = "X", y = "Y") +
  theme(strip.text = element_text(face = "bold",
                                  size = 18)) -> cor_nan_2

print(cor_nan_1)
print(cor_nan_2)
```
:::

::::{.solution}
:::{.cell}

Как отмечалось выше, коэффициент корреляции не определяет напрямую положение линии тренда. Осмыслим обе ситуации, используя формулу для расчета корреляции [-@eq-cor-stand-cov]:

$$
\cor(X, Y) = \frac{\cov(X, Y)}{\sd_X \, \sd_Y}
$$

В случае [-@fig-cor-nan-plot-1] $\cov (X, Y) = 0$ (см. @exr-cov-0). Тогда

$$
\cor(X, Y) = \frac{\cov(X, Y)}{\sd_X  \, \sd_Y} = \frac{0}{\sd_X \, \sd_Y} = 0,
$$

что мы и наблюдаем.

В случае [-@fig-cor-nan-plot-2] $\cov (X, Y) = 0$, как и в предыдущем, но также $Y = \const$. Значит $\var Y = 0$, из чего следует, что $\sd_Y = 0$. Получаем, что

$$
\cor(X, Y) = \frac{\cov(X, Y)}{\sd_X  \, \sd_Y} = \frac{0}{0},
$$

а результат такого выражения не определен --- об этом нам свидетельствует матан. Вообще в R результат такого выражения обозначается как `NaN` (Not a Number), то есть «не-число»:

```{r zero-by-zero-nan}
0 / 0
```

Однако в случае с корреляцией возвращается `NA` (Not Available). Почему было реализовано именно так, неизвестно, но в общем-то трудно спорить с тем, что при такой ситуации значение корреляции «недоступно». Функция расчета корреляции также дополнительно нас оповещает, что причина `NA` в том, что стандартное отклонение равно нулю:

```{r zero-sd-cor, warning=TRUE}
x <- rnorm(500)
y <- rep(0, 500)
cor(x, y)
```
:::
::::



#### Линейность взаимосвязи {#andan-cor-cor-interpret-linearity}

:::{.lab-junior}
:::




#### Некоррелированность и независимость {#andan-cor-cor-interpret-independence}

:::{.lab-junior}
:::




## Коэффициент корреляции Пирсона {#andan-cor-pearson}

:::{.lab-junior}
:::

Мера корреляции, которую мы обсуждали выше, называется **корреляцией Пирсона (Pearson correlation coefficient (PCC), Pearson’s $r$, Pearson product-moment correlation coefficient (PPMCC), bivariate correlation)**.

Преобразовав [-@eq-cor-stand-cov] мы можем получить следующее выражение для вычисления коэффициента корреляции Пирсона:


$$
\cor (X, Y) = \frac{\sum_{i=1}^n (x_i - \overline X)(y_i - \overline Y)}{\sqrt{\sum_{i=1}^n (x_i - \overline X)^2} \cdot \sqrt{\sum_{i=1}^n (y_i - \overline Y)^2}} = r_{X, Y}
$${#eq-cor-pearson-formula}

:::{#exr-cor-pearson-formula}
Получите из формулы [-@eq-cor-stand-cov] формулу [-@eq-cor-pearson-formula].
:::

::::{.solution}
:::{.cell}
$$
\begin{split}
\cor (X, Y) &= \frac{\cov(X, Y)}{\sd_X \, \sd_y} = \\
&= \frac{\dfrac{1}{n-1} \sum_{i=1}^n (x_i - \overline X)(y_i - \overline Y)}{\sqrt{\dfrac{\sum_{i=1}^n (x_i - \overline X)^2}{n-1}} \sqrt{\dfrac{\sum_{i=1}^n (y_i - \overline Y)^2}{n-1}}} = \\
&= \frac{\dfrac{1}{n-1} \sum_{i=1}^n (x_i - \overline X)(y_i - \overline Y)}{\dfrac{1}{n-1}\sqrt{\sum_{i=1}^n (x_i - \overline X)^2} \sqrt{\sum_{i=1}^n (y_i - \overline Y)^2}} = \\
&= \frac{\sum_{i=1}^n (x_i - \overline X)(y_i - \overline Y)}{\sqrt{\sum_{i=1}^n (x_i - \overline X)^2} \sqrt{\sum_{i=1}^n (y_i - \overline Y)^2}}
\end{split}
$$
:::
::::

За формулой [-@eq-cor-pearson-formula] скрывается некоторый интересный смысл, но к нему мы [обратимся позже](#andan-cor-angle-cos), когда будем копаться в математических деталях. Сейчас займемся более насущными вопросами.



### Тестирование статистической значимости коэффициента корреляции {#andan-cor-pearson-sig}

:::{.lab-junior}
:::

$$
\begin{split}
H_0 &: \rho = 0 \\
H_1 &: \rho \neq 0
\end{split}
$$

$$
t = \frac{r_{X,Y}}{\sqrt{\frac{1 - r_{X, Y}^2}{n-2}}} \overset{H_0}{\thicksim} t(n\!-\!2)
$$



### Доверительный интервал для коэффициента корреляции {#andan-cor-pearson-ci}

:::{.lab-junior}
:::


$$
z_{X, Y} = z(r_{X, Y}) = \frac{1}{2} \ln \frac{1 + r_{X, Y}}{1 - r_{X, Y}}
$$

$$
z_{X, Y} \thicksim \norm (z_{X, Y}, \tfrac{1}{n-3})
$$

$$
\se_\text{lazy} = \sqrt{\frac{1 - r^2_{X,Y}}{n-2}}
$$

$$
95\% \text{CI}_\text{lazy} \, [r_{X,Y} - 1.96 \se_\text{lazy}, \, r_{X,Y} + 1.96 \se_\text{lazy}]
$$

$$
\se = \sqrt{\frac1{n-3}}
$$

$$
95\% \text{CI} \, [\tanh(z_{X,Y} - 1.96 \se), \, \tanh(z_{X,Y} + 1.96 \se)]
$$


### Точность оценки коэффициента корреляции

:::{.lab-middle}
:::

```{r cor-se-ci-data, cache=TRUE}
#| code-fold: true
#| code-summary: "Код создания симуляции"

## create function for CI calculation
r_ci <- function(r,
                 n,
                 ci = .95,
                 lazy = FALSE,
                 limits = "both"
                 ) {
  if (lazy) {
    se <- sqrt((1 - r^2) / (n - 2))
    ci_lower <- r + qnorm((1 - ci) / 2) * se
    ci_upper <- r + qnorm(ci + ((1 - ci) / 2)) * se
  } else {
    z_r <- atanh(r)
    se <- 1 / sqrt(n - 3)
    ci_lower <- tanh(z_r + qnorm((1 - ci) / 2) * se)
    ci_upper <- tanh(z_r + qnorm(ci + ((1 - ci) / 2)) * se)
  }
  if (limits == "upper") return(c(upper = ci_upper))
  if (limits == "lower") return(c(lower = ci_lower))
  return(c(lower = ci_lower, 
           upper = ci_upper))
}

tibble(
  r = seq(-.99, .99, by = .01) %>% rep(each = 199),
  n = seq(10, 1000, by = 5) %>% rep(times = 199),
  ci_lower = numeric(39601),
  ci_upper = numeric(39601),
  ci_lower_lazy = numeric(39601),
  ci_upper_lazy = numeric(39601)
) -> r_ci_ds

for (i in 1:nrow(r_ci_ds)) {
  r_ci_ds$ci_lower_lazy[i] <- r_ci(r_ci_ds$r[i], r_ci_ds$n[i], 
                              lazy = TRUE,
                              limits = "lower")
  r_ci_ds$ci_upper_lazy[i] <- r_ci(r_ci_ds$r[i], r_ci_ds$n[i], 
                              lazy = TRUE,
                              limits = "upper")
  r_ci_ds$ci_lower[i] <- r_ci(r_ci_ds$r[i], r_ci_ds$n[i], 
                              lazy = FALSE,
                              limits = "lower")
  r_ci_ds$ci_upper[i] <- r_ci(r_ci_ds$r[i], r_ci_ds$n[i], 
                              lazy = FALSE,
                              limits = "upper")
}
```

```{r cor-se-ci}
#| code-fold: true
#| code-summary: "Код построения визуализации"
#| label: fig-cor-se-ci
#| fig-cap: "Точность оценки корреляции (ширина доверительного интервала) в зависимости от значения коэффициента корреляции и объема выборки"
#| fig-subcap:
#|   - "Способ вычисления «non-lazy»"
#|   - "Способ вычисления «lazy»"
#| layout-ncol: 1

r_ci_ds %>% 
  mutate(has_zero = ifelse(sign(ci_lower) != sign(ci_upper), TRUE, FALSE)) %>% 
  ggplot(aes(x = n, y = r, color = has_zero)) +
  geom_point(size = 1.5, shape = 15) +
  # geom_line(aes(group = group)) +
  scale_color_manual(values = c("TRUE" = "gray20", "FALSE" = "gray80"),
                     labels = c("TRUE" = "включает ноль", "FALSE" = "не включает ноль")) +
  scale_x_continuous(breaks = seq(10, 1000, by = 10)) +
  scale_y_continuous(breaks = seq(-1, 1, by = .05)) +
  labs(x = "Объем выборки", 
       y = "Выборочный коэффициент корреляции",
       color = "Доверительный интервал") +
  theme(axis.text.x = element_text(angle = 90)) -> cor_ci_non_lazy

r_ci_ds %>% 
  mutate(has_zero = ifelse(sign(ci_lower_lazy) != sign(ci_upper_lazy), TRUE, FALSE)) %>% 
  ggplot(aes(x = n, y = r, color = has_zero)) +
  geom_point(size = 1.5, shape = 15) +
  scale_color_manual(values = c("TRUE" = "gray20", "FALSE" = "gray80"),
                     labels = c("TRUE" = "включает ноль", "FALSE" = "не включает ноль")) +
  scale_x_continuous(breaks = seq(10, 1000, by = 10)) +
  scale_y_continuous(breaks = seq(-1, 1, by = .05)) +
  labs(x = "Объем выборки", 
       y = "Выборочный коэффициент корреляции",
       color = "Доверительный интервал") +
  theme(axis.text.x = element_text(angle = 90)) -> cor_ci_lazy

print(cor_ci_non_lazy)
print(cor_ci_lazy)
```


### Размер эффекта для коэффициента корреляции {#andan-cor-effectsize}

:::{.lab-junior}
:::



## Корреляция Пирсона в R {#andan-cor-pearson-in-r}

:::{.lab-junior}
:::

## Коэффициенты корреляции для разных шкал {#andan-cor-scales}

:::{.lab-junior}
:::

:::{#tbl-cors-scales}

| Первая переменная | Вторая переменная | Мера взаимосвязи |
|:---:|:---:|:---:|
| Интервальная или абсолютная | Интервальная или абсолютная | Корреляция Пирсона |
| Ранговая, интервальная или абсолютная | Ранговая, интервальная или абсолютная | Корреляция Спирмена |
| Ранговая | Ранговая | Корреляция Кенделла |
| Интервальная или абсолютная | Дихотомическая | Бисериальный коэффициент |
| Ранговая | Дихотомическая | Рангово-бисериальный коэффициент |
| Дихотомическая | Дихотомическая | $\vphi$-коэффициент |

Меры корреляции для переменных, измеренных в различных шкалах

:::


### Ранговые коэффициенты корреляции {#andan-cor-rank}

:::{.lab-middle}
:::

Коррелировать можно не только сами значения переменных, но и их *ранги*.

Ранговые коэффициенты корреляции инвариантны относительно любого монотонного преобразования переменной.



### Коэффициент корреляции Спирмена {#andan-cor-spearman}

:::{.lab-middle}
:::

$$
\rho = 1 - \frac{6}{n (n-1) (n+1)} \sum_{i=1}^n \big( R_{x_i} - R_{y_i} \big)^2,
$$

где

* $n$ --- объём выборок,
* $R_{x_i}$ --- ранг наблюдения $x_i$ в ряду $X$,
* $R_{y_i}$ --- ранг наблюдения $y_i$ в ряду $Y$.



### Коэффициент корреляции Кенделла {#andan-cor-kendall}

:::{.lab-middle}
:::

$$
\begin{split}
\tau &= 1 - \frac{4}{n (n-1)} R, \\
R &= \sum_{i=1}^{n-1} \sum_{j=i+1}^n \Big[ [x_i < x_j] \neq [y_i < y_j]\Big]
\end{split}
$$




### Точечно-бисериальная корреляция {#andan-cor-pointbiserial}

:::{.lab-middle}
:::

Точечно-бисериальный коэффициент корреляции (point-biserial correlation)

$$
r_{\text{pb}} = \frac{\overline Y_\text{A} - \overline Y_{\text{B}}}{s_Y} \sqrt{\frac{n_\text{A} n_\text{B}}{n(n-1)}},
$${#eq-rpb-s}

где 

* $\overline Y_\text{A}$ --- среднее по переменной $Y$ в группе $\text{A}$,
* $\overline Y_\text{B}$ --- среднее по переменной $Y$ в группе $\text{B}$,
* $n$ --- объём выборки (общее количество наблюдений),
* $n_\text{A}$ --- количество наблюдений в группе $\text{A}$,
* $n_\text{B}$ --- количество наблюдений в группе $\text{B}$,
* $s_Y$ --- выборочное стандартное отклонение по переменной $Y$ (по всей выборке)


Другая формула использует смещённую оценку дисперсии, однако результат получится тот же:

$$
r_{\text{pb}} = \frac{\overline Y_\text{A} - \overline Y_{\text{B}}}{\sigma_Y} \sqrt{\frac{n_\text{A} n_\text{B}}{n^2}},
$${#eq-rpb-sigma}

где

* $\overline Y_\text{A}$ --- среднее по переменной $Y$ в группе $\text{A}$,
* $\overline Y_\text{B}$ --- среднее по переменной $Y$ в группе $\text{B}$,
* $n$ --- объём выборки (общее количество наблюдений),
* $n_\text{A}$ --- количество наблюдений в группе $\text{A}$,
* $n_\text{B}$ --- количество наблюдений в группе $\text{B}$,
* $\sigma_Y$ --- выборочное стандартное отклонение по переменной $Y$ (по всей выборке)


:::{#exr-rpb-equiv}
Покажите, что @eq-rpb-s и @eq-rpb-sigma эквивалентны.
:::

::::{.solution}
:::{.cell}
$$
\begin{split}
r_{\text{pb}} &= 
\frac{
  \overline Y_\text{A} - \overline Y_{\text{B}}
  }{s_Y} 
  \sqrt{\frac{n_\text{A} n_\text{B}}{n(n-1)}} = \\
&= \frac{
  \overline Y_\text{A} - \overline Y_{\text{B}}
  }{
  \sqrt{\dfrac{\sum_{i=1}^n (y_i - \overline Y)}{n-1}}
  } 
  \sqrt{
    \frac{n_\text{A} n_\text{B}}{n(n-1)}
    } = \\
&= \frac{
  \overline Y_\text{A} - \overline Y_{\text{B}}
  }{
  \dfrac{\sqrt{\sum_{i=1}^n (y_i - \overline Y)}}{\sqrt{n-1}}
  } 
  \cdot
  \frac{\sqrt{n_\text{A} n_\text{B}}}{\sqrt{n} \cdot \sqrt{n-1}} = \\
&= \frac{
  \overline Y_\text{A} - \overline Y_{\text{B}}
  }{
  \dfrac{\sqrt{\sum_{i=1}^n (y_i - \overline Y)}}{\sqrt{n-1}}
  } 
  \cdot
  \frac{\sqrt{n_\text{A} n_\text{B}}}{\sqrt{n} \cdot \sqrt{n-1}} 
  \cdot
  \frac{\sqrt{n}}{\sqrt{n-1}}
  \cdot 
  \frac{\sqrt{n-1}}{\sqrt{n}} = \\ 
&= \lp \frac{
  \overline Y_\text{A} - \overline Y_{\text{B}}
  }{
  \dfrac{\sqrt{\sum_{i=1}^n (y_i - \overline Y)}}{\sqrt{n-1}}
  } 
  \cdot
  \frac{\sqrt{n}}{\sqrt{n-1}}
  \rp
  \cdot
  \lp
  \frac{\sqrt{n_\text{A} n_\text{B}}}{\sqrt{n} \cdot \sqrt{n-1}} 
  \cdot 
  \frac{\sqrt{n-1}}{\sqrt{n}} 
  \rp = \\
&= \lp \frac{
  \overline Y_\text{A} - \overline Y_{\text{B}}
  }{
  \dfrac{\sqrt{\sum_{i=1}^n (y_i - \overline Y)}}{\cancel{\sqrt{n-1}}}
  \cdot
  \dfrac{\cancel{\sqrt{n-1}}}{\sqrt{n}}
  } 
  \rp
  \cdot
  \lp
  \frac{\sqrt{n_\text{A} n_\text{B}}}{\sqrt{n} \cdot \cancel{\sqrt{n-1}}} 
  \cdot 
  \frac{\cancel{\sqrt{n-1}}}{\sqrt{n}} 
  \rp = \\
&= \frac{
  \overline Y_\text{A} - \overline Y_{\text{B}}
  }{
  \dfrac{\sqrt{\sum_{i=1}^n (y_i - \overline Y)}}{\sqrt{n}}
  } 
  \cdot
  \frac{\sqrt{n_\text{A} n_\text{B}}}{\sqrt{n} \cdot \sqrt{n}} = \\
&= \frac{
  \overline Y_\text{A} - \overline Y_{\text{B}}
  }{
  \sqrt{
    \dfrac{\sum_{i=1}^n (y_i - \overline Y)}{n}
    }
  } 
  \cdot
  \frac{\sqrt{n_\text{A} n_\text{B}}}{\sqrt{n^2}} = \\
&= \frac{
  \overline Y_\text{A} - \overline Y_{\text{B}}
  }{
  \sigma_Y
  } 
  \cdot
  \sqrt{\frac{n_\text{A} n_\text{B}}{n^2}} \qed
\end{split}
$$
:::
::::




### Рангово-бисериальная корреляция {#andan-cor-rankbiserial}

:::{.lab-middle}
:::

$$
r_{\text{rb}} = \frac{2(\overline Y_\text{A} - \overline Y_\text{B})}{n},
$$

где

* $\overline Y_\text{A}$ --- средний ранг по переменной $Y$ в группе $\text{A}$,
* $\overline Y_\text{B}$ --- средний ранг по переменной $Y$ в группе $\text{B}$,
* $n$ --- объём выборки (общее количество наблюдений)



### $\vphi$-коэффициент {#andan-cor-phi}

:::{.lab-senior}
:::

:::{#tbl-crosstab-phi}
|  | $Y_\text{A}$ | $Y_\text{B}$ |  |
|:---:|:---:|:---:|:---:|
| $X_\text{A}$ | $n_\text{AA}$ | $n_\text{AB}$ | $n_{\text{A} \bullet}$ |
| $X_\text{B}$ | $n_\text{BA}$ | $n_\text{BB}$ | $n_{\text{B} \bullet}$ |
|  | $n_{\bullet \text{A}}$ | $n_{\bullet \text{B}}$ | $n$ |

Таблица сопряжённости для двух дихотомических переменных $X$ и $Y$
:::

$$
\vphi = \frac
{n_\text{AA} n_\text{BB} - n_\text{AB} n_\text{BA}}
{\sqrt{
  n_{\text{A} \bullet}
  n_{\text{B} \bullet}
  n_{\bullet \text{A}}
  n_{\bullet \text{B}}
  }
}
$$


$$
\vphi = \sqrt{\frac{\chi^2}{n}}
$$



### Коэффициент Крамера {#andan-cor-kramer}

:::{.lab-senior}
:::


## Коэффициенты корреляции для разных шкал в R {#andan-cor-scales-in-r}

:::{.lab-junior}
:::







## Угол между векторами {#andan-cor-angle-cos}

:::{.lab-senior}
:::

Не будет большим открытием, что наблюдения по выборке можно представить в виде вектора. Тогда наши переменные (случайные величины) $X$ и $Y$ будут представлять собой векторы $\vm x$ и $\vm y$ в некотором пространстве $\setR^n$, где $n$ --- количество наблюдений:

$$
\begin{split}
\vm x &= \pmatrix{x_1 & x_2 & x_3 & \ldots & x_n} \\
\vm y &= \pmatrix{y_1 & y_2 & y_3 & \ldots & y_n}
\end{split}
$$

Присмотримся поближе к формуле [-@eq-cor-pearson-formula]:

$$
\cor (X, Y) = \frac{\sum_{i=1}^n (x_i - \overline X)(y_i - \overline Y)}{\sqrt{\sum_{i=1}^n (x_i - \overline X)^2} \sqrt{\sum_{i=1}^n (y_i - \overline Y)^2}}
$$

В этой формуле происходит *центрирование* случайных величин $X$ и $Y$ --- часть стандартизации. Введём центрированные случайные величины $\tilde X$ и $\tilde Y$, которые определим как

$$
\begin{split}
\tilde X &= X - \overline X \\
\tilde Y &= Y - \overline Y
\end{split}
\quad \Leftrightarrow \quad
\begin{split}
\tilde x_i &= x_i - \overline X \\
\tilde y_i &= y_i - \overline Y
\end{split}
\quad \Leftrightarrow \quad
\begin{split}
\tilde {\vm x} &= \vm x - \overline X \\
\tilde {\vm y} &= \vm y - \overline Y
\end{split}
$$

Тогда формулу [-@eq-cor-pearson-formula] можно записать в следующем виде:

$$
\cor (X, Y) = \frac{\sum_{i=1}^n \tilde x_i \tilde y_i}{\sqrt{\sum_{i=1}^n \tilde x_i^2} \sqrt{\sum_{i=1}^n \tilde y_i^2}}
$$

Заметим, что

$$
\begin{split}
\sum_{i=1}^n \tilde x_i \tilde y_i &\defin{=} \tilde{\vm x} \cdot \tilde{\vm y} \\
\sqrt{\sum_{i=1}^n \tilde x_i^2} &\defin{=} \|\tilde{\vm x}\| \\
\sqrt{\sum_{i=1}^n \tilde y_i^2} &\defin{=} \|\tilde{\vm y}\|
\end{split}
$$

Тогда 

$$
\cor(X,Y) = \frac{ \tilde{\vm x} \cdot \tilde{\vm y} }{ \| \tilde{\vm x} \| \cdot \| \tilde{\vm y} \| } \defin{=} \cos \theta,
$${#eq-cor-angle-cos}

где $\theta$ --- угол между векторами $\tilde{\vm x}$ и $\tilde{\vm y}$.

После такого поворота событий сразу можно обнаружить несколько соответствий:

:::{.callout-tip appearance="minimal"}

* коэффициент корреляции изменяется в пределах $[-1, 1]$ --- косинус угла изменяется ровно в этих же пределах
* косинус равен нулю, когда угол между векторами равен $90^\circ$, то есть когда векторы *ортогональны* --- иначе говоря, *линейно независимы*
* косинус находится в пределах $(0, 1)$, когда угол между векторами меньше $90^\circ$, то есть когда векторы *сонаправлены*
* косинус находится в пределах $(-1, 0)$, когда угол между векторами больше $90^\circ$, то есть когда векторы *направлены в противоположные стороны*
* косинус *по модулю* равен $1$, когда векторы *коллинеарны* --- иначе говоря, *линейно зависимы*
    * косинус равен $1$, когда угол между векторами равен нулю, то есть когда векторы *коллинеарны* и *сонаправлены*
    * косинус равен $-1$, когда угол между векторами равен $180^\circ$, то есть когда векторы *коллинеарны* и *направлены в противоположные стороны* (*антиколлинеарны*)

:::



## Детали доверительного интервала {#andan-cor-ci-details}

:::{.lab-senior}
:::

### Преобразование Фишера {#andan-cor-fisher-transform}

:::{.lab-senior}
:::


$$
z_{r_i} = \frac{1}{2} \ln \frac{1 + r_i}{1 - r_i} = \artanh r_i
$$

$$
r_\text{P} = \frac{e^{2z_\text{P}} - 1}{e^{2z_\text{P}} + 1} = \tanh z_\text{P}
$$






## Эквивалентность различных коэффициентов корреляции и коэффициента корреляции Пирсона {#andan-cor-pearsonequiv}

:::{.lab-senior}
:::

### Корреляция Спирмена и корреляция Пирсона {#andan-cor-spearman-pearsonequiv}

:::{.lab-senior}
:::

Если мы имеем две выборки нормально распределенных случайных величин, то корреляция Спирмена $\rho$ и корреляция Пирсона $r$ оказываются связаны следующим соотношением:

$$
r = 2 \sin \frac{\pi}{6} \rho
$$





### Точечно-бисериальная корреляция и корреляция Пирсона {#andan-cor-pointbiser-pearsonequiv}

:::{.lab-senior}
:::

$$
\begin{split}
\frac{
  \sum_{i=1}^n (x_i - \overline X) (y_i - \overline Y)
  }{n}&= \\
&= \frac{
  \sum_{i=1}^n (x_i y_i - x_i \overline Y - \overline X y_i + \overline X \cdot \overline Y)
  }{n} = \\
&= \frac{
  \sum_{i=1}^n x_i y_i - \sum_{i=1}^n x_i \overline Y - \sum_{i=1}^n \overline X y_i + \sum_{i=1}^n (\overline X \cdot \overline Y)
  }{n} = \\
&= \frac{
  \sum_{i \,:\, x_i = 1}^n y_i - n_1 \overline Y - \overline X \sum_{i=1}^n y_i + n \cdot \overline X \cdot \overline Y
  }{n} = \\
&= \frac{\sum_{i \,:\, x_i = 1}^n y_i}{n} - 
    \frac{n_1 \overline Y}{n} - 
    \frac{\overline X \sum_{i=1}^n y_i}{n} + 
    \frac{n \cdot \overline X \cdot \overline Y}{n} = \\
&= \overline Y_1 - \overline X \cdot \overline Y - \overline X \cdot \overline Y + \overline X \cdot \overline Y ???????????
\end{split}
$$



$$
\begin{split}
\sd_X &\defin{=} \sqrt{\frac{\sum_{i=1}^n (x_i - \overline X)^2}{n-1}} = \\
&= \sqrt{\frac{\sum_{i=1}^n x_i^2 - \sum_{i=1}^n 2\overline X x_i + \sum_{i=1}^n \overline X^2}{n-1}} = \\
&= \sqrt{\frac{\sum_{i=1}^n x_i - 2\overline X \sum_{i=1}^n x_i + \sum_{i=1}^n \overline X^2}{n-1}} = \\
&= \sqrt{\frac{n_1 - 2 \dfrac{n_1}{n} n_1 + n \dfrac{n_1^2}{n^2}}{n-1}} = \\
&= \sqrt{\frac{nn_1 - 2 \dfrac{n_1}{n} n_1n + n^2 \dfrac{n_1^2}{n^2}}{n(n-1)}} = \\
&= \sqrt{\frac{n_1(n - 2 n_1 + n_1)}{n(n-1)}} = \\
&= \sqrt{\frac{n_1(n - n_1)}{n(n-1)}} = \\
&= \sqrt{\frac{n_1 n_0}{n(n-1)}}
\end{split}
$$


***

###### Session Info {#session_info .unnumbered}

```{r session-info}
sessionInfo()
```

```{=html}
<script type="text/javascript" src="./js/chapter.js"></script>
```
