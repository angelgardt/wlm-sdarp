# Корреляционный анализ {#andan-cor}

{{< include other/_symbols.qmd >}}

```{r opts, echo=FALSE, eval=TRUE, message=FALSE, warning=FALSE, error=FALSE}
knitr::opts_chunk$set(echo=TRUE, eval=TRUE, message=FALSE, warning=FALSE, error=FALSE)
```

```{r pkgs, echo=FALSE}
library(tidyverse)
theme_set(theme_bw())
```

:::{.intro}
При изучении [описательной статистики](part-desc.qmd) мы рассматривали отдельные переменные и описывали их поведение на нашей выборке. Однако в ситуации реального исследования нас интересуют взаимосвязи между различными переменными --- это то, ради чего мы затеваем наше исследование. Мы хотим узнать, как различные переменные связаны друг с другом, и ровно это мы и закладываем в наши гипотезы. Что ж, давайте начнём знакомиться с инструментами, которые позволят получить ответы на подобные вопросы.

Не будет преувеличением сказать, что корреляция есть база и основа всех методов анализа данных. Именно поэтому мы начинаем с неё. Итак, в путь!
:::


## Совместная изменчивость признаков {#andan-cor-covariance}

:::{.lab-junior}
:::

Пусть у нас есть две непрерывные переменные (случайные величины) $X$ и $Y$. Мы можем визуализировать их взаимосвязь, расположив имеющиеся у нас наблюдения в осях $X$ и $Y$. Иначе говоря, мы можем построить следующую визуализацию (@fig-cor-scheme-pos-start).

![Схема визуализации взаимосвязи между двумя непрерывными переменными](img/andan-cor/cor-scheme-pos-start.jpg){#fig-cor-scheme-pos-start}

Связь между переменными отчетливо опознается --- с ростом значений одной переменной растут значения другой переменной. Мы даже можем примерно провести некоторую прямую, описывающую такую взаимосвязь (@fig-cor-scheme-pos-trend).

![Связь легко схватывается визуально](img/andan-cor/cor-scheme-pos-trend.jpg){#fig-cor-scheme-pos-trend}

Но пока оставим прямую в покое. Как бы нам описать имеющуюся ситуацию математически? Ну, пожалуй, не будет большим открытием, если я скажу, что мы можем посчитать средние [арифметические] по обеим переменным. Действительно, среднее --- это одна из описательных статистик, и имея ряд наблюдений мы запросто сможем его посчитать. Добавим на нашу визуализацию средние по обеим переменным (@fig-cor-scheme-pos-means).

![Средние по обеим переменным на визуализации](img/andan-cor/cor-scheme-pos-means.jpg){#fig-cor-scheme-pos-means}

Чем нам это поможет? Средние разбили наши наблюдения на четыре части. То, в какой из частей будет находится конкретное наблюдение, описывается его отклонением по обеим переменным (@fig-cor-scheme-pos-deviations). Заметим, что у нас получается:

* много *сонаправленных отклонений* --- таких наблюдений, для которых $(x - \overline X) > 0 \wedge (y - \overline Y) > 0$ или $(x - \overline X) < 0 \wedge (y - \overline Y) < 0$;.
* мало *разнонаправленных отклонений* --- таких наблюдений, для которых $(x - \overline X) < 0 \wedge (y - \overline Y) > 0$ или $(x - \overline X) < 0 \wedge (y - \overline Y) > 0$.

![Отклонения от средних по обеим переменным](img/andan-cor/cor-scheme-pos-deviations.jpg){#fig-cor-scheme-pos-deviations}

Как это можно использовать для описания закономерности? Давайте обратим внимание на знак произведения отклонений в каждой из частей наблюдений (@fig-cor-scheme-pos-signs):

* если отклонения *сонаправлены*, то их произведение *положительно*:

$$
\begin{split}
\overset{\circled{+}}{(x - \overline X)} \overset{\circled{+}}{(y - \overline Y)} &> 0 \\
\overset{\circled{-}}{(x - \overline X)} \overset{\circled{-}}{(y - \overline Y)} &> 0
\end{split}
$${#eq-deviation-same-direct}

* если отклонения *разнонаправлены*, то их произведение *отрицательно*:

$$
\begin{split}
\overset{\circled{+}}{(x - \overline X)} \overset{\circled{-}}{(y - \overline Y)} &< 0 \\
\overset{\circled{-}}{(x - \overline X)} \overset{\circled{+}}{(y - \overline Y)} &< 0
\end{split}
$${#eq-deviation-diff-direct}

![Знаки произведений отклонений по обеим переменным](img/andan-cor/cor-scheme-pos-signs.jpg){#fig-cor-scheme-pos-signs}

Отлично! Но нас мало интересуют отклонения для конкретных наблюдений --- нам хочется узнать, как дело обстоит в целом, то есть какова (со)направленность отклонений *в среднем*. Со средним мы знакомы, давайте его посчитаем:

$$
\frac{1}{n} \sum_{i=1}^n (x - \overline X)(y - \overline Y) > 0,
$${#eq-mean-codeviation-gzero}

где $n$ --- количество наблюдений (для обеих переменных оно одинаково).

По сути @eq-mean-codeviation-gzero описывает среднее отклонение наблюдений по двум переменным. Так как сонаправленных отклонений у нас больше, то среднее будет положительно.


Хорошо. А будет ли это работать для случая, когда связь между переменными будет *обратной*? Попробуем изобразить такой случай (@fig-cor-scheme-neg-start).

![Схема визуализации обратной взаимосвязи между двумя непрерывными переменными](img/andan-cor/cor-scheme-neg-start.jpg){#fig-cor-scheme-neg-start}

Вновь связь между переменными отчетливо опознается --- с ростом значений одной переменной значения другой переменной уменьшаются. И мы вновь можем примерно провести некоторую прямую, описывающую такую взаимосвязь (@fig-cor-scheme-neg-trend).

![Обратная связь также легко схватывается визуально](img/andan-cor/cor-scheme-neg-trend.jpg){#fig-cor-scheme-neg-trend}

Опять же, оставим пока прямую, и добавим средние по нашим переменным на визуализацию (@fig-cor-scheme-neg-means). Они, безусловно, как и в предыдущем случае разбьют наши наблюдения на четыре части.

![Средние по обеим переменным на визуализации обратной связи](img/andan-cor/cor-scheme-neg-means.jpg){#fig-cor-scheme-neg-means}

Однако в этом случае у нас получится, что (@fig-cor-scheme-neg-deviations):

* много *разнонаправленных отклонений* --- таких наблюдений, для которых $(x - \overline X) < 0 \wedge (y - \overline Y) > 0$ или $(x - \overline X) < 0 \wedge (y - \overline Y) > 0$.
* мало *сонаправленных отклонений* --- таких наблюдений, для которых $(x - \overline X) > 0 \wedge (y - \overline Y) > 0$ или $(x - \overline X) < 0 \wedge (y - \overline Y) < 0$;.

![Отклонения от средних по обеим переменным при обратной связи ](img/andan-cor/cor-scheme-neg-deviations.jpg){#fig-cor-scheme-neg-deviations}

Имея в виду знаки произведений отклонений ([-@eq-deviation-same-direct] и [-@eq-deviation-diff-direct], @fig-cor-scheme-neg-signs), можем заметить, что в этом случае среднее отклонение наблюдений по двум переменным будет отрицательным:

$$
\frac{1}{n} \sum_{i=1}^n (x - \overline X)(y - \overline Y) < 0,
$${#eq-mean-codeviation-lzero}

![Знаки произведений отклонений по обеим переменным при обратной связи](img/andan-cor/cor-scheme-neg-signs.jpg){#fig-cor-scheme-neg-signs}

:::{.callout-note appearance="minimal"}
Мы получили некое выражение ([-@eq-mean-codeviation-gzero] и [-@eq-mean-codeviation-lzero]), по знаку которого мы можем судить о направлении связи между переменными --- прямая ([-@fig-cor-scheme-pos-start]) или обратная ([-@fig-cor-scheme-neg-start]).
:::

:::{#exr-cov-0}
Мы рассмотрели выше случаи, когда связь между переменными (случайными величинами) есть. Однако вполне возможна ситуация, когда связь между переменными отсутствует.

Постройте схематичную визуализацию для такого случая и на основе неё выясните, что нам скажет найденное нами выражение.
:::

::::{.solution}
:::{.cell}

Ситуацию отсутствия взаимосвязи между двумя переменными можно представить так:

![Схема визуализации отсутствия взаимосвязи между двумя непрерывными переменными](img/andan-cor/cor-scheme-zero-start.jpg)

Добавив знаки отклонений от средних значений, увидим, что сонаправленных и разнонаправленных отклонений оказывается примерно поровну.

![Знаки произведений отклонений по обеим переменным при отсутствии связи](img/andan-cor/cor-scheme-zero-signs.jpg)

Следовательно, в этом случае

$$
\frac{1}{n} \sum_{i=1}^n (x - \overline X)(y - \overline Y) \approx 0
$$

:::
::::



## Ковариация {#andan-cor-cov}

:::{.lab-junior}
:::

По сути, в предыдущем разделе мы изучали *ковариацию*.

:::{#def-cov}
Ковариация --- мера совместной изменчивости двух переменных (случайных величин).
:::

Само название **ковариация (covariance)** обозначает **совместную** (**ко-**) **изменчивость** (**-вариацию**)[^co-variance].

[^co-variance]: А *вариация* (*variance*) --- это другое название дисперсии, как мы обсуждали ранее.

Более того, мы практически получили формулу расчета ковариации двух переменных --- @eq-mean-codeviation-gzero (и [-@eq-mean-codeviation-lzero]). Однако полученные нами формулы будут давать смещённую оценку ковариации. Причина абсолютно аналогична той, что была в дисперсии --- мы используем выборочные средние, чтобы рассчитать отклонения. Для того, чтобы получить несмещенную оценку ковариации, необходимо скорректировать формулу --- делить не на $n$ (количество наблюдений), а на $n-1$. Тогда получим, что ковариацию двух переменных можно вычислить по имеющимся наблюдениям по формуле:

$$
\cov(X, Y) = \frac{1}{n-1} \sum_{i=1}^n (x_i - \overline X) (y_i - \overline Y),
$${#eq-cov-sample}

где $\cov(X, Y)$ --- ковариация переменных $X$ и $Y$, $n$ --- количество наблюдений, $\overline X$ и $\overline Y$ --- выборочные средние.


### Интерпретация значения ковариации {#andan-cor-cov-interpret}

:::{.lab-junior}
:::

Наша поправка в формуле [-@eq-cov-sample] не повлияет на наблюдения, произведенные выше. Мы отметили, что

:::{.callout-important appearance="minimal"}

* в случае **прямой взаимосвязи** между переменными **ковариация положительна**,
* в случае **обратной взаимосвязи** между переменными **ковариация отрицательна**,
* в случае **отсутствия взаимосвязи** между переменными **ковариация равна нулю** (см. @exr-cov-0).

:::

Таким образом, теперь мы можем определять **направление** связи на основе ковариации.

Можно ли что-либо сказать о *силе связи* на основе ковариации? Тут хорошо бы сначала определиться с тем, что вообще такое сила связи. Не совсем понятно, как это определить строго, но давайте посмотрим на следующие картинки (@fig-cov-var).

```{r cov-var-data}
#| code-fold: true
#| code-summary: "Код создания симуляции"

n <- 100
set.seed(616)
tibble(
  x1 = rnorm(n, sd = 1),
  y1 = 0.4 * x1 + rnorm(n, mean = 2, sd = 1 - 0.4^2),
  x2 = rnorm(n, sd = 2),
  y2 = 0.4 * x2 + rnorm(n, mean = 2, sd = 2 - 2 * 0.4^2),
  x3 = rnorm(n, sd = 1),
  y3 = -0.8 * x3 + rnorm(n, mean = 2, sd = 1 - 0.8^2),
  x4 = rnorm(n, sd = 4),
  y4 = -0.8 * x4 + rnorm(n, mean = 4, sd = 4 - 4 * 0.8^2)
) -> sim_cov


sim_cov %>% 
  ggplot(aes(x1, y1)) +
  geom_point() +
  geom_smooth(method = "lm", 
              se = FALSE,
              color = "gray60") +
  labs(x = "X", y = "Y") -> cov1

sim_cov %>% 
  ggplot(aes(x2, y2)) +
  geom_point() +
  geom_smooth(method = "lm", 
              se = FALSE,
              color = "gray60") +
  labs(x = "X", y = "Y") -> cov2

sim_cov %>% 
  ggplot(aes(x3, y3)) +
  geom_point() +
  geom_smooth(method = "lm", 
              se = FALSE,
              color = "gray60") +
  labs(x = "X", y = "Y") -> cov3

sim_cov %>% 
  ggplot(aes(x4, y4)) +
  geom_point() +
  geom_smooth(method = "lm",
              se = FALSE,
              color = "gray60") +
  labs(x = "X", y = "Y") -> cov4
```

```{r cov-var-plots, echo=FALSE}
#| code-fold: true
#| code-summary: "Код построения визуализации"
#| label: fig-cov-var
#| fig-cap: "Визуализация симуляций при разных параметрах распределений случайных величин $X$ и $Y$ ($n = 100$)"
#| fig-subcap:
#|   - "$X_1 \\thicksim \\norm(0, 1)$, $Y_1 \\thicksim \\norm(2, ???)$"
#|   - "$X_2 \\thicksim \\norm(0, 4)$, $Y_2 \\thicksim \\norm(2, ???)$"
#|   - "$X_3 \\thicksim \\norm(0, 1)$, $Y_3 \\thicksim \\norm(2, ???)$"
#|   - "$X_4 \\thicksim \\norm(0, 16)$, $Y_4 \\thicksim \\norm(4, ???)$"
#| layout-ncol: 2

print(cov1)
print(cov2)
print(cov3)
print(cov4)
```

Видно, что связи между $X$ и $Y$ на первых двух визуализациях ([-@fig-cov-var-1] и [-@fig-cov-var-2]) прямые, а на двух вторых ([-@fig-cov-var-3] и [-@fig-cov-var-4]) --- обратные. При этом также визуально мы можем сказать, что обратные связи *сильнее*, чем прямые --- наблюдения активнее группируются вокруг прямой, описывающей закономерность. Однако если сравнивать между собой обе прямые или обе обратные связи, то по силе они приблизительно равны.

Попробуем посчитать ковариацию между переменными в четырех имеющихся ситуациях:

```{r cov-var-covs}
#| code-fold: true
#| code-summary: "Код расчета ковариаций"
#| label: tbl-cov-var-values
#| tbl-cap: Значения ковариаций в симуляции

sim_cov %>% 
  summarise(
    `$$\\text{cov}(X_1, Y_1)$$` = cov(x1, y1),
    `$$\\text{cov}(X_2, Y_2)$$` = cov(x2, y2),
    `$$\\text{cov}(X_3, Y_3)$$` = cov(x3, y3),
    `$$\\text{cov}(X_4, Y_4)$$` = cov(x4, y4)
  ) %>% 
  knitr::kable(format = "html",
               digits = 2,
               align = "c")
```

Наблюдаем нечто интересное: визуально связи между $X_1$ и $Y_1$ ([-@fig-cov-var-1]) и между $X_2$ и $Y_2$ ([-@fig-cov-var-2]) примерно равны по силе, однако значения ковариаций различаются: $\cov(X_1, Y_1) =$ `r round(cov(sim_cov$x1, sim_cov$y1), 2)`, $\cov(X_2, Y_2) =$ `r round(cov(sim_cov$x2, sim_cov$y2), 2)`. Конечно, значения различаются не сильно --- может быть это не так существенно? Посмотрим на обратные связи. Визуально связи между $X_3$ и $Y_3$ ([-@fig-cov-var-3]) и между $X_4$ и $Y_4$ ([-@fig-cov-var-4]) также примерно равны по силе, однако значения ковариаций существенно различаются --- на целый порядок: $\cov(X_3, Y_3) =$ `r round(cov(sim_cov$x3, sim_cov$y3), 2)`, $\cov(X_4, Y_4) =$ `r round(cov(sim_cov$x4, sim_cov$y4), 2)`.

Почему же ковариация так сильно различается? Всё дело в дисперсии. Давайте рассчитаем дисперсию переменных и сопоставим её с ковариацией (@tbl-cov-var-vars). Мы видим, что в случаях, где дисперсия переменных выше, ковариация оказывается также выше.

```{r cov-var-vars}
#| code-fold: true
#| code-summary: "Код расчета вариаций"
#| label: tbl-cov-var-vars
#| tbl-cap: Значения вариаций и ковариаций в симуляции

sim_cov %>% 
  summarise(
    # `$$\\overline X$$__$$a$$` = mean(x1),
    # `$$\\overline Y$$__$$a$$` = mean(y1),
    # `$$\\overline X$$__$$b$$` = mean(x2),
    # `$$\\overline Y$$__$$b$$` = mean(y2),
    # `$$\\overline X$$__$$c$$` = mean(x3),
    # `$$\\overline Y$$__$$c$$` = mean(y3),
    # `$$\\overline X$$__$$d$$` = mean(x4),
    # `$$\\overline Y$$__$$d$$` = mean(y4),
    `$$\\text{var} \\,X$$__$$a$$` = var(x1),
    `$$\\text{var} \\,Y$$__$$a$$` = var(y1),
    `$$\\text{var} \\,X$$__$$b$$` = var(x2),
    `$$\\text{var} \\,Y$$__$$b$$` = var(y2),
    `$$\\text{var} \\,X$$__$$c$$` = var(x3),
    `$$\\text{var} \\,Y$$__$$c$$` = var(y3),
    `$$\\text{var} \\,X$$__$$d$$` = var(x4),
    `$$\\text{var} \\,Y$$__$$d$$` = var(y4),
    `$$\\text{cov} (X,Y)$$__$$a$$` = cov(x1, y1),
    `$$\\text{cov} (X,Y)$$__$$b$$` = cov(x2, y2),
    `$$\\text{cov} (X,Y)$$__$$c$$` = cov(x3, y3),
    `$$\\text{cov} (X,Y)$$__$$d$$` = cov(x4, y4)
  ) %>% 
  pivot_longer(cols = everything()) %>% 
  separate(name, into = c("stat", "Визуализация"), sep = "__") %>% 
  pivot_wider(names_from = stat, values_from = value) %>% 
  knitr::kable(format = "html",
               digits = 2,
               align = "c")
```

Итак, мы пронаблюдали, что

:::{.callout-important appearance="minimal"}
по значению ковариации мы не можем судить о силе связи, так как оно зависит от дисперсии переменных
:::

И всё же сила связи нам в исследованиях была бы очень интересна --- нам необходима какая-то новая статистика, лишенная этого недостатка ковариации.



## Корреляция {#andan-cor-correlation}

:::{.lab-junior}
:::








```{=html}
<script type="text/javascript" src="./js/chapter.js"></script>
```
