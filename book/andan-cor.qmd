# Корреляционный анализ {#andan-cor}

{{< include other/_symbols.qmd >}}

```{r opts, echo=FALSE, eval=TRUE, message=FALSE, warning=FALSE, error=FALSE}
knitr::opts_chunk$set(echo=TRUE, eval=TRUE, message=FALSE, warning=FALSE, error=FALSE)
```

```{r pkgs, echo=FALSE}
library(tidyverse)
theme_set(theme_bw())
```

:::{.intro}
При изучении [описательной статистики](part-desc.qmd) мы рассматривали отдельные переменные и описывали их поведение на нашей выборке. Однако в ситуации реального исследования нас интересуют взаимосвязи между различными переменными --- это то, ради чего мы затеваем наше исследование. Мы хотим узнать, как различные переменные связаны друг с другом, и ровно это мы и закладываем в наши гипотезы. Что ж, давайте начнём знакомиться с инструментами, которые позволят получить ответы на подобные вопросы.

Не будет преувеличением сказать, что корреляция есть база и основа всех методов анализа данных. Именно поэтому мы начинаем с неё. Итак, в путь!
:::


## Совместная изменчивость признаков {#andan-cor-covariance}

:::{.lab-junior}
:::

Пусть у нас есть две непрерывные переменные (случайные величины) $X$ и $Y$. Мы можем визуализировать их взаимосвязь, расположив имеющиеся у нас наблюдения в осях $X$ и $Y$. Иначе говоря, мы можем построить следующую визуализацию (@fig-cor-scheme-pos-start).

![Схема визуализации взаимосвязи между двумя непрерывными переменными](img/andan-cor/cor-scheme-pos-start.jpg){#fig-cor-scheme-pos-start}

Связь между переменными отчетливо опознается --- с ростом значений одной переменной растут значения другой переменной. Мы даже можем примерно провести некоторую прямую, описывающую такую взаимосвязь (@fig-cor-scheme-pos-trend).

![Связь легко схватывается визуально](img/andan-cor/cor-scheme-pos-trend.jpg){#fig-cor-scheme-pos-trend}

Но пока оставим прямую в покое. Как бы нам описать имеющуюся ситуацию математически? Ну, пожалуй, не будет большим открытием, если я скажу, что мы можем посчитать средние [арифметические] по обеим переменным. Действительно, среднее --- это одна из описательных статистик, и имея ряд наблюдений мы запросто сможем его посчитать. Добавим на нашу визуализацию средние по обеим переменным (@fig-cor-scheme-pos-means).

![Средние по обеим переменным на визуализации](img/andan-cor/cor-scheme-pos-means.jpg){#fig-cor-scheme-pos-means}

Чем нам это поможет? Средние разбили наши наблюдения на четыре части. То, в какой из частей будет находится конкретное наблюдение, описывается его отклонением по обеим переменным (@fig-cor-scheme-pos-deviations). Заметим, что у нас получается:

* много *сонаправленных отклонений* --- таких наблюдений, для которых $(x - \overline X) > 0 \wedge (y - \overline Y) > 0$ или $(x - \overline X) < 0 \wedge (y - \overline Y) < 0$;.
* мало *разнонаправленных отклонений* --- таких наблюдений, для которых $(x - \overline X) < 0 \wedge (y - \overline Y) > 0$ или $(x - \overline X) < 0 \wedge (y - \overline Y) > 0$.

![Отклонения от средних по обеим переменным](img/andan-cor/cor-scheme-pos-deviations.jpg){#fig-cor-scheme-pos-deviations}

Как это можно использовать для описания закономерности? Давайте обратим внимание на знак произведения отклонений в каждой из частей наблюдений (@fig-cor-scheme-pos-signs):

* если отклонения *сонаправлены*, то их произведение *положительно*:

$$
\begin{split}
\overset{\circled{+}}{(x - \overline X)} \overset{\circled{+}}{(y - \overline Y)} &> 0 \\
\overset{\circled{-}}{(x - \overline X)} \overset{\circled{-}}{(y - \overline Y)} &> 0
\end{split}
$${#eq-deviation-same-direct}

* если отклонения *разнонаправлены*, то их произведение *отрицательно*:

$$
\begin{split}
\overset{\circled{+}}{(x - \overline X)} \overset{\circled{-}}{(y - \overline Y)} &< 0 \\
\overset{\circled{-}}{(x - \overline X)} \overset{\circled{+}}{(y - \overline Y)} &< 0
\end{split}
$${#eq-deviation-diff-direct}

![Знаки произведений отклонений по обеим переменным](img/andan-cor/cor-scheme-pos-signs.jpg){#fig-cor-scheme-pos-signs}

Отлично! Но нас мало интересуют отклонения для конкретных наблюдений --- нам хочется узнать, как дело обстоит в целом, то есть какова (со)направленность отклонений *в среднем*. Со средним мы знакомы, давайте его посчитаем:

$$
\frac{1}{n} \sum_{i=1}^n (x - \overline X)(y - \overline Y) > 0,
$${#eq-mean-codeviation-gzero}

где $n$ --- количество наблюдений (для обеих переменных оно одинаково).

По сути @eq-mean-codeviation-gzero описывает среднее отклонение наблюдений по двум переменным. Так как сонаправленных отклонений у нас больше, то среднее будет положительно.


Хорошо. А будет ли это работать для случая, когда связь между переменными будет *обратной*? Попробуем изобразить такой случай (@fig-cor-scheme-neg-start).

![Схема визуализации обратной взаимосвязи между двумя непрерывными переменными](img/andan-cor/cor-scheme-neg-start.jpg){#fig-cor-scheme-neg-start}

Вновь связь между переменными отчетливо опознается --- с ростом значений одной переменной значения другой переменной уменьшаются. И мы вновь можем примерно провести некоторую прямую, описывающую такую взаимосвязь (@fig-cor-scheme-neg-trend).

![Обратная связь также легко схватывается визуально](img/andan-cor/cor-scheme-neg-trend.jpg){#fig-cor-scheme-neg-trend}

Опять же, оставим пока прямую, и добавим средние по нашим переменным на визуализацию (@fig-cor-scheme-neg-means). Они, безусловно, как и в предыдущем случае разбьют наши наблюдения на четыре части.

![Средние по обеим переменным на визуализации обратной связи](img/andan-cor/cor-scheme-neg-means.jpg){#fig-cor-scheme-neg-means}

Однако в этом случае у нас получится, что (@fig-cor-scheme-neg-deviations):

* много *разнонаправленных отклонений* --- таких наблюдений, для которых $(x - \overline X) < 0 \wedge (y - \overline Y) > 0$ или $(x - \overline X) < 0 \wedge (y - \overline Y) > 0$.
* мало *сонаправленных отклонений* --- таких наблюдений, для которых $(x - \overline X) > 0 \wedge (y - \overline Y) > 0$ или $(x - \overline X) < 0 \wedge (y - \overline Y) < 0$;.

![Отклонения от средних по обеим переменным при обратной связи ](img/andan-cor/cor-scheme-neg-deviations.jpg){#fig-cor-scheme-neg-deviations}

Имея в виду знаки произведений отклонений ([-@eq-deviation-same-direct] и [-@eq-deviation-diff-direct], @fig-cor-scheme-neg-signs), можем заметить, что в этом случае среднее отклонение наблюдений по двум переменным будет отрицательным:

$$
\frac{1}{n} \sum_{i=1}^n (x - \overline X)(y - \overline Y) < 0,
$${#eq-mean-codeviation-lzero}

![Знаки произведений отклонений по обеим переменным при обратной связи](img/andan-cor/cor-scheme-neg-signs.jpg){#fig-cor-scheme-neg-signs}

:::{.callout-note appearance="minimal"}
Мы получили некое выражение ([-@eq-mean-codeviation-gzero] и [-@eq-mean-codeviation-lzero]), по знаку которого мы можем судить о направлении связи между переменными --- прямая ([-@fig-cor-scheme-pos-start]) или обратная ([-@fig-cor-scheme-neg-start]).
:::

:::{#exr-cov-0}
Мы рассмотрели выше случаи, когда связь между переменными (случайными величинами) есть. Однако вполне возможна ситуация, когда связь между переменными отсутствует.

Постройте схематичную визуализацию для такого случая и на основе неё выясните, что нам скажет найденное нами выражение.
:::

::::{.solution}
:::{.cell}

Ситуацию отсутствия взаимосвязи между двумя переменными можно представить так:

![Схема визуализации отсутствия взаимосвязи между двумя непрерывными переменными](img/andan-cor/cor-scheme-zero-start.jpg)

Добавив знаки отклонений от средних значений, увидим, что сонаправленных и разнонаправленных отклонений оказывается примерно поровну.

![Знаки произведений отклонений по обеим переменным при отсутствии связи](img/andan-cor/cor-scheme-zero-signs.jpg)

Следовательно, в этом случае

$$
\frac{1}{n} \sum_{i=1}^n (x - \overline X)(y - \overline Y) \approx 0
$$

:::
::::



## Ковариация {#andan-cor-cov}

:::{.lab-junior}
:::

По сути, в предыдущем разделе мы изучали *ковариацию*.

:::{#def-cov}
Ковариация --- мера совместной изменчивости двух переменных (случайных величин).
:::

Само название **ковариация (covariance)** обозначает **совместную** (**ко-**) **изменчивость** (**-вариацию**)[^co-variance].

[^co-variance]: А *вариация* (*variance*) --- это другое название дисперсии, как мы обсуждали ранее.

Более того, мы практически получили формулу расчета ковариации двух переменных --- @eq-mean-codeviation-gzero (и [-@eq-mean-codeviation-lzero]). Однако полученные нами формулы будут давать смещённую оценку ковариации. Причина абсолютно аналогична той, что была в дисперсии --- мы используем выборочные средние, чтобы рассчитать отклонения. Для того, чтобы получить несмещенную оценку ковариации, необходимо скорректировать формулу --- делить не на $n$ (количество наблюдений), а на $n-1$. Тогда получим, что ковариацию двух переменных можно вычислить по имеющимся наблюдениям по формуле:

$$
\cov(X, Y) = \frac{1}{n-1} \sum_{i=1}^n (x_i - \overline X) (y_i - \overline Y),
$${#eq-cov-sample}

где $\cov(X, Y)$ --- ковариация переменных $X$ и $Y$, $n$ --- количество наблюдений, $\overline X$ и $\overline Y$ --- выборочные средние.

:::{#exr-cov-xx}
Чему будет равна ковариация переменной с самой собой, то есть $\cov(X, X)$?
:::

::::{.solution}
:::{.cell}
$$
\begin{split}
\cov(X,X) &= \frac{1}{n-1}\sum_{i=1}^n (x_i - \overline X)(x_i - \overline X) = \\
&= \frac{1}{n-1} \sum_{i=1}^n (x_i - \overline X)^2 \defin{=} \var X
\end{split}
$$
:::
::::


### Интерпретация значения ковариации {#andan-cor-cov-interpret}

:::{.lab-junior}
:::

Наша поправка в формуле [-@eq-cov-sample] не повлияет на наблюдения, произведенные выше. Мы отметили, что

:::{.callout-important appearance="minimal"}

* в случае **прямой взаимосвязи** между переменными **ковариация положительна**,
* в случае **обратной взаимосвязи** между переменными **ковариация отрицательна**,
* в случае **отсутствия взаимосвязи** между переменными **ковариация равна нулю** (см. @exr-cov-0).

:::

Таким образом, теперь мы можем определять **направление** связи на основе ковариации.

Можно ли что-либо сказать о *силе связи* на основе ковариации? Тут хорошо бы сначала определиться с тем, что вообще такое сила связи. Не совсем понятно, как это определить строго, но давайте посмотрим на следующие картинки (@fig-cov-var).

```{r cov-var-data}
#| code-fold: true
#| code-summary: "Код создания симуляции"

n <- 100
set.seed(616)
tibble(
  x1 = rnorm(n, sd = 1),
  y1 = 0.4 * x1 + rnorm(n, mean = 2, sd = 1 - 0.4^2),
  x2 = rnorm(n, sd = 2),
  y2 = 0.4 * x2 + rnorm(n, mean = 2, sd = 2 - 2 * 0.4^2),
  x3 = rnorm(n, sd = 1),
  y3 = -0.8 * x3 + rnorm(n, mean = 2, sd = 1 - 0.8^2),
  x4 = rnorm(n, sd = 4),
  y4 = -0.8 * x4 + rnorm(n, mean = 4, sd = 4 - 4 * 0.8^2)
) -> sim_cov


sim_cov %>% 
  ggplot(aes(x1, y1)) +
  geom_point() +
  geom_smooth(method = "lm", 
              se = FALSE,
              color = "gray60") +
  labs(x = "X", y = "Y") -> cov1

sim_cov %>% 
  ggplot(aes(x2, y2)) +
  geom_point() +
  geom_smooth(method = "lm", 
              se = FALSE,
              color = "gray60") +
  labs(x = "X", y = "Y") -> cov2

sim_cov %>% 
  ggplot(aes(x3, y3)) +
  geom_point() +
  geom_smooth(method = "lm", 
              se = FALSE,
              color = "gray60") +
  labs(x = "X", y = "Y") -> cov3

sim_cov %>% 
  ggplot(aes(x4, y4)) +
  geom_point() +
  geom_smooth(method = "lm",
              se = FALSE,
              color = "gray60") +
  labs(x = "X", y = "Y") -> cov4
```

```{r cov-var-plots, echo=FALSE}
#| code-fold: true
#| code-summary: "Код построения визуализации"
#| label: fig-cov-var
#| fig-cap: "Визуализация симуляций при разных параметрах распределений случайных величин $X$ и $Y$ ($n = 100$)"
#| fig-subcap:
#|   - "$X_1 \\thicksim \\norm(0, 1)$, $Y_1 \\thicksim \\norm(2, ???)$"
#|   - "$X_2 \\thicksim \\norm(0, 4)$, $Y_2 \\thicksim \\norm(2, ???)$"
#|   - "$X_3 \\thicksim \\norm(0, 1)$, $Y_3 \\thicksim \\norm(2, ???)$"
#|   - "$X_4 \\thicksim \\norm(0, 16)$, $Y_4 \\thicksim \\norm(4, ???)$"
#| layout-ncol: 2

print(cov1)
print(cov2)
print(cov3)
print(cov4)
```

Видно, что связи между $X$ и $Y$ на первых двух визуализациях ([-@fig-cov-var-1] и [-@fig-cov-var-2]) прямые, а на двух вторых ([-@fig-cov-var-3] и [-@fig-cov-var-4]) --- обратные. При этом также визуально мы можем сказать, что обратные связи *сильнее*, чем прямые --- наблюдения активнее группируются вокруг прямой, описывающей закономерность. Однако если сравнивать между собой обе прямые или обе обратные связи, то по силе они приблизительно равны.

Попробуем посчитать ковариацию между переменными в четырех имеющихся ситуациях:

```{r cov-var-covs}
#| code-fold: true
#| code-summary: "Код расчета ковариаций"
#| label: tbl-cov-var-values
#| tbl-cap: Значения ковариаций в симуляции

sim_cov %>% 
  summarise(
    `$$\\text{cov}(X_1, Y_1)$$` = cov(x1, y1) %>% round(2) %>% paste0("$$", ., "$$"),
    `$$\\text{cov}(X_2, Y_2)$$` = cov(x2, y2) %>% round(2) %>% paste0("$$", ., "$$"),
    `$$\\text{cov}(X_3, Y_3)$$` = cov(x3, y3) %>% round(2) %>% paste0("$$", ., "$$"),
    `$$\\text{cov}(X_4, Y_4)$$` = cov(x4, y4) %>% round(2) %>% paste0("$$", ., "$$")
  ) %>% 
  knitr::kable(format = "html",
               align = "c")
```

Наблюдаем нечто интересное: визуально связи между $X_1$ и $Y_1$ ([-@fig-cov-var-1]) и между $X_2$ и $Y_2$ ([-@fig-cov-var-2]) примерно равны по силе, однако значения ковариаций различаются: $\cov(X_1, Y_1) =$ `r round(cov(sim_cov$x1, sim_cov$y1), 2)`, $\cov(X_2, Y_2) =$ `r round(cov(sim_cov$x2, sim_cov$y2), 2)`. Конечно, значения различаются не сильно --- может быть это не так существенно? Посмотрим на обратные связи. Визуально связи между $X_3$ и $Y_3$ ([-@fig-cov-var-3]) и между $X_4$ и $Y_4$ ([-@fig-cov-var-4]) также примерно равны по силе, однако значения ковариаций существенно различаются --- на целый порядок: $\cov(X_3, Y_3) =$ `r round(cov(sim_cov$x3, sim_cov$y3), 2)`, $\cov(X_4, Y_4) =$ `r round(cov(sim_cov$x4, sim_cov$y4), 2)`.

Почему же ковариация так сильно различается? Всё дело в дисперсии. Давайте рассчитаем дисперсию переменных и сопоставим её с ковариацией (@tbl-cov-var-vars). Мы видим, что в случаях, где дисперсия переменных выше, ковариация оказывается также выше.

```{r cov-var-vars}
#| code-fold: true
#| code-summary: "Код расчета вариаций"
#| label: tbl-cov-var-vars
#| tbl-cap: Значения вариаций и ковариаций в симуляции

sim_cov %>% 
  summarise(
    # `$$\\overline X$$__$$a$$` = mean(x1),
    # `$$\\overline Y$$__$$a$$` = mean(y1),
    # `$$\\overline X$$__$$b$$` = mean(x2),
    # `$$\\overline Y$$__$$b$$` = mean(y2),
    # `$$\\overline X$$__$$c$$` = mean(x3),
    # `$$\\overline Y$$__$$c$$` = mean(y3),
    # `$$\\overline X$$__$$d$$` = mean(x4),
    # `$$\\overline Y$$__$$d$$` = mean(y4),
    `$$\\text{var} \\,X$$__a` = var(x1) %>% round(2) %>% paste0("$$", ., "$$"),
    `$$\\text{var} \\,Y$$__a` = var(y1) %>% round(2) %>% paste0("$$", ., "$$"),
    `$$\\text{var} \\,X$$__b` = var(x2) %>% round(2) %>% paste0("$$", ., "$$"),
    `$$\\text{var} \\,Y$$__b` = var(y2) %>% round(2) %>% paste0("$$", ., "$$"),
    `$$\\text{var} \\,X$$__c` = var(x3) %>% round(2) %>% paste0("$$", ., "$$"),
    `$$\\text{var} \\,Y$$__c` = var(y3) %>% round(2) %>% paste0("$$", ., "$$"),
    `$$\\text{var} \\,X$$__d` = var(x4) %>% round(2) %>% paste0("$$", ., "$$"),
    `$$\\text{var} \\,Y$$__d` = var(y4) %>% round(2) %>% paste0("$$", ., "$$"),
    `$$\\text{cov} (X,Y)$$__a` = cov(x1, y1) %>% round(2) %>% paste0("$$", ., "$$"),
    `$$\\text{cov} (X,Y)$$__b` = cov(x2, y2) %>% round(2) %>% paste0("$$", ., "$$"),
    `$$\\text{cov} (X,Y)$$__c` = cov(x3, y3) %>% round(2) %>% paste0("$$", ., "$$"),
    `$$\\text{cov} (X,Y)$$__d` = cov(x4, y4) %>% round(2) %>% paste0("$$", ., "$$")
  ) %>% 
  pivot_longer(cols = everything()) %>% 
  separate(name, into = c("stat", "Визуализация"), sep = "__") %>% 
  pivot_wider(names_from = stat, values_from = value) %>% 
  knitr::kable(format = "html",
               align = "c") %>% 
  kableExtra::column_spec(1, extra_css = 'vertical-align: middle !important;') %>% 
  kableExtra::row_spec(0, extra_css = 'vertical-align: middle !important;')
```

Итак, мы пронаблюдали, что

:::{.callout-important appearance="minimal"}
по значению ковариации мы не можем судить о силе связи, так как оно зависит от дисперсии переменных
:::

И всё же сила связи нам в исследованиях была бы очень интересна --- нам необходима какая-то новая статистика, лишенная этого недостатка ковариации.



## Корреляция {#andan-cor-correlation}

:::{.lab-junior}
:::

Перед нами стоит задача избавиться от влияния дисперсии на значение ковариации. Но избавляться от дисперсии мы уже умеем --- в этом нам поможет **стандартизация**.

Мы можем перед расчетом ковариации между переменными $X$ и $Y$ стандартизировать эти переменные. Получим новые переменные $X^\star$ и $Y^\star$, которые будут определяться как

$$
\begin{split}
X^\star &= \frac{X - \overline X}{\sd_X} \\
Y^\star &= \frac{Y - \overline Y}{\sd_Y}
\end{split}
$$

Дисперсия этих переменных будет равна единице, а попутно мы ещё получим среднее, равное нулю. Теперь мы можем рассчитать ковариацию между ними --- получим **корреляцию** между переменными $X$ и $Y$:

$$
\cov(X^\star, Y^\star) = \frac{1}{n-1} \sum_{i=1}^n (x^\star_i - \overline X^\star)(y^\star_i - \overline Y^\star) = \frac{1}{n-1} \sum_{i=1}^n x^\star_i y^\star_i \defin{=} \cor(X, Y)
$${#eq-cor-stand-vars}

:::{#def-cor-stand-vars}
Коэффициентом корреляции называется ковариация стандартизированных случайных величин.
:::

Мы можем не стандартизировать переменные изначально, а выполнить преобразования по ходу расчета корреляции. Тогда получим, что

$$
\begin{split}
\cor (X, Y) &\defin{=} \cov(X^\star, Y^\star) = \\
&= \frac{1}{n-1} \sum_{i=1}^n \frac{x_i - \overline X}{\sd_X} \cdot \frac{y_i - \overline Y}{\sd_Y} = \\
&= \frac{1}{n-1} \sum_{i=1}^n \frac{1}{\sd_X \, \sd_Y} (x_i - \overline X)(y_i - \overline Y) = \\
&= \frac{1}{\sd_X \, \sd_Y} \lp \frac{1}{n-1} \sum_{i=1}^n (x_i - \overline X) (y_i - \overline Y) \rp = \\
&= \frac{\cov(X,Y)}{\sd_X \, \sd_Y}
\end{split}
$${#eq-cor-stand-cov}

:::{#def-cor-stand-cov}
Коэффициентом корреляции называется стандартизированное значение ковариации.
:::

### Интерпретация значения коэффициента корреляции {#andan-cor-cor-interpret}

:::{.lab-junior}
:::



:::{#def-correlation}
Корреляция --- мера линейной взаимосвязи двух переменных (случайных величин).
:::


## Коэффициент корреляции Пирсона {#andan-cor-pearson}

:::{.lab-junior}
:::

Преобразовав [-@eq-cor-stand-cov] мы можем получить следующее выражение для вычисления коэффициента корреляции Пирсона:

$$
\cor (X, Y) = \frac{\sum_{i=1}^n (x_i - \overline X)(y_i - \overline Y)}{\sqrt{\sum_{i=1}^n (x_i - \overline X)^2} \sqrt{\sum_{i=1}^n (y_i - \overline Y)^2}}
$${#eq-cor-pearson-formula}

:::{#exr-cor-pearson-formula}
Получите из формулы [-@eq-cor-stand-cov] формулу [-@eq-cor-pearson-formula].
:::

::::{.solution}
:::{.cell}
$$
\begin{split}
\cor (X, Y) &= \frac{\cov(X, Y)}{\sd_X \, \sd_y} = \\
&= \frac{\dfrac{1}{n-1} \sum_{i=1}^n (x_i - \overline X)(y_i - \overline Y)}{\sqrt{\dfrac{\sum_{i=1}^n (x_i - \overline X)^2}{n-1}} \sqrt{\dfrac{\sum_{i=1}^n (y_i - \overline Y)^2}{n-1}}} = \\
&= \frac{\dfrac{1}{n-1} \sum_{i=1}^n (x_i - \overline X)(y_i - \overline Y)}{\dfrac{1}{n-1}\sqrt{\sum_{i=1}^n (x_i - \overline X)^2} \sqrt{\sum_{i=1}^n (y_i - \overline Y)^2}} = \\
&= \frac{\sum_{i=1}^n (x_i - \overline X)(y_i - \overline Y)}{\sqrt{\sum_{i=1}^n (x_i - \overline X)^2} \sqrt{\sum_{i=1}^n (y_i - \overline Y)^2}}
\end{split}
$$
:::
::::

За формулой [-@eq-cor-pearson-formula] скрывается некоторый интересный смысл, но к нему мы [обратимся позже](#andan-cor-angle-cos), когда будем копаться в математических деталях. Сейчас займемся более насущными вопросами.

### Тестирование статистической значимости коэффициента корреляции {#andan-cor-pearson-sig}

:::{.lab-junior}
:::

$$
\begin{split}
H_0 &: \rho = 0 \\
H_1 &: \rho \neq 0
\end{split}
$$

### Доверительный интервал для коэффициента корреляции {#andan-cor-pearson-ci}

:::{.lab-junior}
:::

### Размер эффекта для коэффициента корреляции {#andan-cor-effectsize}

:::{.lab-junior}
:::

## Корреляция Пирсона в R {#andan-cor-pearson-in-r}

:::{.lab-junior}
:::

## Коэффициенты корреляции для разных шкал {#andan-cor-scales}

:::{.lab-junior}
:::

:::{#tbl-cors-scales}

| Первая переменная | Вторая переменная | Мера взаимосвязи |
|:---:|:---:|:---:|
| Интервальная или абсолютная | Интервальная или абсолютная | Корреляция Пирсона |
| Ранговая, интервальная или абсолютная | Ранговая, интервальная или абсолютная | Корреляция Спирмена |
| Ранговая | Ранговая | Корреляция Кенделла |
| Интервальная или абсолютная | Дихотомическая | Бисериальный коэффициент |
| Ранговая | Дихотомическая | Рагново-бисериальный коэффициент |
| Дихотомическая | Дихотомическая | $\vphi$-коэффициент |

Меры корреляции для переменных, измеренных в различных шкалах

:::


### Коэффициент корреляции Спирмена {#andan-cor-spearman}

:::{.lab-middle}
:::

### Коэффициент корреляции Кенделла {#andan-cor-kendall}

:::{.lab-middle}
:::

### Бисериальный коэффициент корреляции {#andan-cor-biserial}

:::{.lab-middle}
:::

### Рангово-бисериальный коэффициент корреляции {#andan-cor-rank-biserial}

:::{.lab-middle}
:::

### $\vphi$-коэффициент {#andan-cor-phi}

:::{.lab-middle}
:::


## Коэффициенты корреляции для разных шкал в R {#andan-cor-scales-in-r}

:::{.lab-junior}
:::







## Угол между векторами {#andan-cor-angle-cos}

:::{.lab-senior}
:::

Не будет большим открытием, что наблюдения по выборке можно представить в виде вектора. Тогда наши переменные (случайные величины) $X$ и $Y$ будут представлять собой векторы $\vm x$ и $\vm y$ в некотором пространстве $\setR^n$, где $n$ --- количество наблюдений:

$$
\begin{split}
\vm x &= \pmatrix{x_1 & x_2 & x_3 & \ldots & x_n} \\
\vm y &= \pmatrix{y_1 & y_2 & y_3 & \ldots & y_n}
\end{split}
$$

Присмотримся поближе к формуле [-@eq-cor-pearson-formula]:

$$
\cor (X, Y) = \frac{\sum_{i=1}^n (x_i - \overline X)(y_i - \overline Y)}{\sqrt{\sum_{i=1}^n (x_i - \overline X)^2} \sqrt{\sum_{i=1}^n (y_i - \overline Y)^2}}
$$

В этой формуле происходит *центрирование* случайных величин $X$ и $Y$ --- часть стандартизации. Введём центрированные случайные величины $\tilde X$ и $\tilde Y$, которые определим как

$$
\begin{split}
\tilde X &= X - \overline X \\
\tilde Y &= Y - \overline Y
\end{split}
\quad \Leftrightarrow \quad
\begin{split}
\tilde x_i &= x_i - \overline X \\
\tilde y_i &= y_i - \overline Y
\end{split}
\quad \Leftrightarrow \quad
\begin{split}
\tilde {\vm x} &= \vm x - \overline X \\
\tilde {\vm y} &= \vm y - \overline Y
\end{split}
$$

Тогда формулу [-@eq-cor-pearson-formula] можно записать в следующем виде:

$$
\cor (X, Y) = \frac{\sum_{i=1}^n \tilde x_i \tilde y_i}{\sqrt{\sum_{i=1}^n \tilde x_i^2} \sqrt{\sum_{i=1}^n \tilde y_i^2}}
$$

Заметим, что

$$
\begin{split}
\sum_{i=1}^n \tilde x_i \tilde y_i &\defin{=} \tilde{\vm x} \cdot \tilde{\vm y} \\
\sqrt{\sum_{i=1}^n \tilde x_i^2} &\defin{=} \|\tilde{\vm x}\| \\
\sqrt{\sum_{i=1}^n \tilde y_i^2} &\defin{=} \|\tilde{\vm y}\|
\end{split}
$$

Тогда 

$$
\cor(X,Y) = \frac{ \tilde{\vm x} \cdot \tilde{\vm y} }{ \| \tilde{\vm x} \| \cdot \| \tilde{\vm y} \| } \defin{=} \cos \theta,
$${#eq-cor-angle-cos}

где $\theta$ --- угол между векторами $\tilde{\vm x}$ и $\tilde{\vm y}$.

После такого поворота событий сразу можно обнаружить несколько соответствий:

:::{.callout-tip appearance="minimal"}

* коэффициент корреляции изменяется в пределах $[-1, 1]$ --- косинус угла изменяется ровно в этих же пределах
* косинус равен нулю, когда угол между векторами равен $90^\circ$, то есть когда векторы *ортогональны* --- иначе говоря, *линейно независимы*
* косинус находится в пределах $(0, 1)$, когда угол между векторами меньше $90^\circ$, то есть когда векторы *сонаправлены*
* косинус находится в пределах $(-1, 0)$, когда угол между векторами больше $90^\circ$, то есть когда векторы *направлены в противоположные стороны*
* косинус *по модулю* равен $1$, когда векторы *коллинеарны* --- иначе говоря, *линейно зависимы*
    * косинус равен $1$, когда угол между векторами равен нулю, то есть когда векторы *коллинеарны* и *сонаправлены*
    * косинус равен $-1$, когда угол между векторами равен $180^\circ$, то есть когда векторы *коллинеарны* и *направлены в противоположные стороны* (*антиколлинеарны*)

:::



## Детали доверительного интервала {#andan-cor-ci-details}

:::{.lab-senior}
:::

### Преобразование Фишера {#andan-cor-fisher-transform}

:::{.lab-senior}
:::



***

###### Session Info {#session_info .unnumbered}

```{r session-info}
sessionInfo()
```

```{=html}
<script type="text/javascript" src="./js/chapter.js"></script>
```
