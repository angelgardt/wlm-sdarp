# Простая линейная регрессия {#andan-simplelinear}

{{< include other/_symbols.qmd >}}

```{r opts, echo=FALSE, eval=TRUE, message=FALSE, warning=FALSE, error=FALSE}
knitr::opts_chunk$set(echo=TRUE, eval=TRUE, message=FALSE, warning=FALSE, error=FALSE)
```

:::{.intro}
Вступление
:::


## Идея линейной регрессии {#andan-simplelinear-idea}

## Математическая модель {#andan-simplelinear-model}

$$
y_i = \beta_0 + \beta_1 x_i + \vepsilon_i
$$

$$
y_i = \hat \beta_0 + \hat \beta_1 x_i + \hat \vepsilon_i = b_0 + b_1 x_i + e_i
$$

$$
\hat y_i = b_0 + b_1 x_i
$$

## Тестирование качества модели {#andan-simplelinear-test}

### Структура изменчивости данных с точки зрения регрессионной модели {#andan-simplelinear-test-datavar}

$$
\TSS = \sum_{i=1}^n (y_i - \overline Y)^2
$$

$$
\ESS = \sum_{i=1}^n (\hat y_i - \overline Y)^2
$$

$$
\RSS = \sum_{i=1}^n (y_i - \hat y_i)^2
$$


$$
\MS_\text{t} = \frac{\TSS}{n-1} = \frac{\sum_{i=1}^n (y_i - \overline Y)^2}{n-1} = \hat \sigma^2_Y = s^2_Y
$$

$$
\MS_\text{e} = \hat \sigma^2_\text{e}
$$

$$
\MS_\text{r} = \hat \sigma^2_\text{r}
$$


### F-критерий {#andan-simplelinear-test-fisher}

$$
\begin{split}
H_0 &: \sigma_\text{e} \leq \sigma_\text{r} \\
H_1 &: \sigma_\text{e} > \sigma_\text{r}
\end{split}
$$


$$
F = \frac{\MS_\text{e}}{\MS_\text{r}} = \frac{\hat \sigma^2_\text{e}}{\hat \sigma^2_\text{r}}
\disted{H_0} F(\df_1, \df_2)
$$


## Тестирование значимости коэффициентов {#andan-simplelinear-test-student}


$$
\begin{split}
H_0 &: \beta_1 = 0 \\
H_1 &: \beta_1 \neq 0
\end{split}
$$

$$
t = \frac{b_1 - \beta_1}{\se_{b_1}} \overset{H_0}{=} \frac{b_1}{\se_{b_1}} \disted{H_0} t(n-2)
$$

$$
\se_{b_1} = \frac{\sqrt{\frac{1}{n-2} \sum_{i=1}^n (y_i - \hat y_i)^2}}{\sqrt{\sum_{i=1}^n (x_i - \overline x)^2}}
$$

$$
t = \frac{r \sqrt{n-2}}{\sqrt{1 - r^2}}
$$



## Математика простой линейной регрессии {#andan-simplelinear-math}

### Аналитическое вычисление коэффициентов {#andan-simplelinear-solve-deriv}

$$
L(b_0, b_1) = \sum_{i=1}^n e_i^2 = \sum_{i=1}^n (y_i - \hat y_i)^2 = \sum_{i=1}^n (y_i - b_0 - b_1x_i)^2 \rightarrow \min_{b_0, b_1}
$$

$$
\begin{split}
L(b_0, b_1) &= \sum_{i=1}^n (y_i - b_0 - b_1x_i) (y_i - b_0 - b_1x_i) \\
L(b_0, b_1) &= \sum_{i=1}^n (y_i^2 - b_0 y_i - b_1 x_i y_i - b_0 y_i - b_1 x_i y_i + b_0 b_1 x_i + b_1^2 x_i^2 + b_0^2 + b_0 b_1 x_i) \\
L(b_0, b_1) &= \sum_{i=1}^n (y_i^2 - 2 b_1 x_i y_i - 2 y_i b_0 + x_i^2 b_1^2 + b_0^2 + 2 x_i b_1 b_0)
\end{split}
$$

$$
\begin{split}
\frac{\partial L(b_0, b_1)}{\partial b_0} &= 
\sum_{i=1}^n (-2y_i + 2b_0 + 2x_ib_1) = 
-2 \sum_{i=1}^n \big( y_i - (b_0 + b_1 x_i) \big) \\
\frac{\partial L(b_0, b_1)}{\partial b_1} &= 
\sum_{i=1}^n (-2 x_i y_i + 2 x_i^2 b_1 + 2 x_i b_0) = 
-2 \sum_{i=1}^n \big( y_i - (b_0 + b_1 x_i) \big) x_i
\end{split}
$$

$$
\begin{split}
&\cases {
-2 \sum_{i=1}^n \big( y_i - (b_0 + b_1 x_i) \big) = 0 \\
-2 \sum_{i=1}^n \big( y_i - (b_0 + b_1 x_i) \big) x_i = 0
} \\
&\cases{
\sum_{i=1}^n \big( y_i - (b_0 + b_1 x_i) \big) = 0 \\
\sum_{i=1}^n \big( y_i - (b_0 + b_1 x_i) \big) x_i = 0
} \\
&\cases{
\sum_{i=1}^n y_i - \sum_{i=1}^n b_0 + \sum_{i=1}^n b_1 x_i = 0 \\
\sum_{i=1}^n y_i x_i - \sum_{i=1}^n b_0 x_i + \sum_{i=1}^n b_1 x^2_i = 0
} \\
&\cases{
\sum_{i=1}^n b_0 + \sum_{i=1}^n b_1 x_i = \sum_{i=1}^n y_i \\
\sum_{i=1}^n b_0 x_i + \sum_{i=1}^n b_1 x_i^2 = \sum_{i=1}^n y_i x_i
} \\
&\cases{
n b_0 + b_1 \sum_{i=1}^n x_i = \sum_{i=1}^n y_i \\
b_0 \sum_{i=1}^n x_i + b_1 \sum_{i=1}^n x^2_i = \sum_{i=1}^n y_i x_i
}
\end{split}
$$

$$
\begin{split}
b_0 &= \frac{\sum_{i=1}^n y_i}{n} - b_1 \frac{\sum_{i=1}^n x_i}{n} \\
b_0 &= \overline Y - b_1 \overline X
\end{split}
$$

$$
\begin{split}
\underline{b_1 \sum_{i=1}^n x_i^2} + 
\overline Y \sum_{i=1}^n x_i - 
\underline{b_1 \overline X \sum_{i=1}^n x_i} = 
\sum_{i=1}^n x_i y_i \\
b_1 \Big( \sum_{i=1}^n x_i^2 - \overline X \sum_{i=1}^n x_i \Big) = 
\sum_{i=1}^n x_i y_i - \overline Y \sum_{i=1}^n x_i
\end{split}
$$

$$
\begin{split}
b_1 &= 
\frac{\sum_{i=1}^n x_i y_i - \overline Y \sum_{i=1}^n x_i}
{\sum_{i=1}^n x_i^2 - \overline X \sum_{i=1}^n x_i} = 
\frac{ \dfrac{\sum_{i=1}^n x_i y_i - \overline Y \sum_{i=1}^n x_i}{n}}
{ \dfrac{\sum_{i=1}^n x_i^2 - \overline X \sum_{i=1}^n x_i}{n}} \\
b_1 &= 
\frac{\overline{XY} - \overline X \cdot \overline Y}
{\overline{X^2} - \overline X^2} = 
\frac{\overline{XY} - \overline X \cdot \overline Y}
{\sigma_X^2}
\end{split}
$$

Итак, зафиксируем результаты --- коэффициенты линейной регрессии вычисляются по следующим формулам:

$$
b_0 = \overline Y - b_1 \overline X
$${#eq-b0-calc}

$$
b_1 = \frac{\overline{XY} - \overline X \cdot \overline Y}
{\overline{X^2} - \overline X^2} = 
\frac{\overline{XY} - \overline X \cdot \overline Y}
{\sigma_X^2}
$${#eq-b1-calc}

### Матричное представление модели {#andan-simplelinear-model-matrix}

$$
y_i = b_0 + b_1 x_i + e_i
$$

$$
\begin{cases}
y_1 = b_0 + b_1 x_1 + e_1 \\
y_2 = b_0 + b_1 x_2 + e_2 \\
y_3 = b_0 + b_1 x_3 + e_3 \\
\vdots \\
y_n = b_0 + b_1 x_n + e_n
\end{cases}
$$


$$
\vm y = \vm X \vm b + \vm e
$$

### Матричное вычисление коэффициентов {#andan-simplelinear-solve-matrix}

$$
L(\vm b) = \|\vm y - \vm X \vm b \| ^2 = \tp{(\vm y - \vm X \vm b)} (\vm y - \vm X \vm b) \to \min_{\vm b}
$$

$$
L(\vm b) = \tp{\vm y} \vm y - 2 \tp{\vm y} \vm X \vm b + \tp{\vm b} \tp{\vm X} \vm X \vm b
$$
$$
\frac{\partial L(\vm b)}{\partial \vm b} = \frac{\partial (\tp{\vm y} \vm y - 2 \tp{\vm y} \vm X \vm b + \tp{\vm b} \tp{\vm X} \vm X \vm b)}{\partial \vm b} = -2 \tp{\vm X} \vm y + 2 \tp{\vm X} \vm X \vm b
$$

$$
\begin{split}
-2 \tp{\vm X} \vm y + 2 \tp{\vm X} \vm X \vm b = 0 \\
-2 (\tp{\vm X} \vm y - \tp{\vm X} \vm X \vm b) = 0 \\
\tp{\vm X} \vm y - \tp{\vm X} \vm X \vm b = 0 \\
\tp{\vm X} \vm X \vm b = \tp{\vm X} \vm y
\end{split}
$$

$$
\vm b = (\tp{\vm X} \vm X)^{-1} \tp{\vm X} \vm y
$$


### Угловой коэффициент и коэффициент корреляции {#andan-simplelinear-solve-cor}

:::{.lab-senior}
:::

Ранее мы отмечали, что коэффициент корреляции [связан с наклоном линии тренда](andan-cor.qmd#andan-cor-cor-interpret-slope), который, как мы теперь знаем, определяется угловым коэффициентом регрессии. Тогда, несомненно, мы можем утверждать, что коэффициент корреляции связан с угловым коэффициентов регрессии. Более того, мы можем сформулировать математически, как организована данная связь (@prp-cor-slope).

:::{#prp-cor-slope}
Коэффициент корреляции Пирсона связан с угловым коэффициентом простой линейной регрессии следующим соотношением:

$$
b_1 = r_{X,Y} \cdot \frac{s_Y}{s_X},
$$

где $r_{XY}$ --- коэффициент корреляции Пирсона между переменными $X$ и $Y$,
$s_X$ --- выборочное стандартное отклонение переменной $X$,
$s_Y$ --- выборочное стандартное отклонение переменной $Y$,
$b_1$ --- угловой коэффициент регрессионной модели, задаваемой уравнением $\hat y_i = b_0 + b_1 x_i$ ($x_i$ и $y_i$ --- наблюдения по переменным $X$ и $Y$ соответственно).

:::

:::{.proof}
Как мы выяснили выше, угловой коэффициент регрессии вычисляется так (@eq-b1-calc):

$$
b_1 = \frac{\overline{XY} - \overline X \cdot \overline Y}
{\sigma_X^2}.
$$

Тогда 

$$
\begin{split}
b_1 &= \frac{ \dfrac{\sum_{i=1}^n x_i y_i}{n} - \dfrac{\overline Y \sum_{i=1}^n x_i}{n} }{ \dfrac{\sum_{i=1}^n (x_i - \overline X)^2}{n} } = \\
&= \frac{ \dfrac{\sum_{i=1}^n x_i y_i - \overline Y \sum_{i=1}^n x_i}{n} }{ \dfrac{\sum_{i=1}^n (x_i - \overline X)^2}{n} } = \\
&= \frac{\sum_{i=1}^n x_i y_i - \overline Y \sum_{i=1}^n x_i}{\sum_{i=1}^n (x_i - \overline X)^2} = \\
&= \frac{\sum_{i=1}^n (x_i y_i - x_i \overline Y)}{\sum_{i=1}^n (x_i - \overline X)^2} = \\
&= \frac{\sum_{i=1}^n x_i (y_i - \overline Y)}{\sum_{i=1}^n (x_i - \overline X)^2} = \\
&= \frac{\sum_{i=1}^n x_i (y_i - \overline Y) - \overline X \overset{=0}{ \boxed{\sum_{i=1}^n (y_i - \overline Y)}}}{\sum_{i=1}^n (x_i - \overline X)^2} = \\
&= \frac{\sum_{i=1}^n x_i (y_i - \overline Y) - \sum_{i=1}^n \overline X (y_i - \overline Y)}{\sum_{i=1}^n (x_i - \overline X)^2} = \\
&= \frac{\sum_{i=1}^n \big( x_i (y_i - \overline Y) - \overline X (y_i - \overline Y) \big)}{\sum_{i=1}^n (x_i - \overline X)^2} = \\
&= \frac{\sum_{i=1}^n (x_i - \overline X)(y_i - \overline Y)}{\sum_{i=1}^n (x_i - \overline X)^2} = \\
&= \frac{\sum_{i=1}^n (x_i - \overline X)(y_i - \overline Y)}{ \sqrt{\sum_{i=1}^n (x_i - \overline X)^2} \cdot \sqrt{\sum_{i=1}^n (x_i - \overline X)^2} } \cdot \overset{=1}{ \boxed{ \frac{\sqrt{\sum_{i=1}^n (y_i - \overline Y)^2}}{\sqrt{\sum_{i=1}^n (y_i - \overline Y)^2}} } } = \\
&= \frac{\sum_{i=1}^n (x_i - \overline X)(y_i - \overline Y)}
{\sqrt{\sum_{i=1}^n (x_i - \overline X)^2} \cdot \sqrt{\sum_{i=1}^n (y_i - \overline Y)^2}}
\cdot
\frac{\sqrt{\sum_{i=1}^n (y_i - \overline Y)^2}}
{\sqrt{\sum_{i=1}^n (x_i - \overline X)^2}} = \\
&= r_{XY}
\cdot
\frac{\sqrt{\sum_{i=1}^n (y_i - \overline Y)^2}}
{\sqrt{\sum_{i=1}^n (x_i - \overline X)^2}} = \\
&= r_{XY}
\cdot
\frac{\sqrt{\sum_{i=1}^n (y_i - \overline Y)^2} \cdot \sqrt{\dfrac{1}{n-1}}}
{\sqrt{\sum_{i=1}^n (x_i - \overline X)^2} \cdot \sqrt{\dfrac{1}{n-1}}} = \\
&= r_{XY}
\cdot
\frac{\sqrt{\dfrac{\sum_{i=1}^n (y_i - \overline Y)^2}{n-1}}}
{\sqrt{\dfrac{\sum_{i=1}^n (x_i - \overline X)^2}{n-1}}} = \\
&= r_{XY} \cdot \frac{s_Y}{s_X} \qed
\end{split}
$$

:::

:::{#cor-equals-slope}
Если регрессионная модель построена на стандартизированных или нормированных переменных, то угловой коэффициент модели равен коэффициенту корреляции между переменными.
:::

:::{.proof}
Если переменные $X$ и $Y$ были стандартизированы или нормированы, то их выборочные стандартные отклонения $s_X$ и $s_Y$ равны единице. Следовательно,

$$
b_1 = r_{XY} \cdot \frac{s_X}{s_Y} = r_{XY} \cdot \frac{1}{1} = r_{XY}
$$

:::


### TSS = ESS + RSS?

:::{.lab-guru}
:::

:::{.quote .small .pers}
--- Назрел вопросик. Ты выше сказал, что общая сумма квадратов --- это объяснённая сумма квадратов плюс остаточная сумма квадратов. Но на картинке отображены не квадраты, а разницы между реальными значениями, модельными значениями и средним. Для них равенство очевидно. А вот будет ли так работать с суммами квадратов --- совершенно не факт! Что-то ты, автор, не договариваешь…<br>
--- Ты прав. Давай разбираться.
:::

$$
\begin{split}
d_\text{t} &= y_i - \overline Y \\
d_\text{e} &= \hat y_i - \overline Y \\
d_\text{t} &= y_i - \hat y_i
\end{split}
$$

$$
\begin{split}
d_\text{t} &= y_i - \overline Y = \\
&= y_i - \overline Y + \hat y_i - \hat y_i = \\
&= (\hat y_i - \overline Y) + (y_i - \hat y_i) = \\
&= d_\text{e} + d_\text{r}
\end{split}
$$

Однако я утверждаю ([-@prp-tss]), что

:::{#prp-tss}
$$
\TSS = \ESS + \RSS
$$
:::

:::{.proof}
$$
\begin{split}
\TSS &= \sum_{i=1}^n (y_i - \overline Y)^2 = \\
&= \sum_{i=1}^n (y_i - \hat y + \hat y - \overline Y)^2 = \\
&= \sum_{i=1}^n \big( (y_i - \hat y_i) + (\hat y_i - \overline Y) \big)^2 = \\
&= \sum_{i=1}^n (y_i - \hat y_i)^2 + \sum_{i=1}^n (\hat y_i - \overline Y)^2 + 2 \sum_{i=1}^n (y_i - \hat y_i)(\hat y_i - \overline Y) = \\
&= \RSS + \ESS + 2 \sum_{i=1}^n (y_i - \hat y_i)(\hat y_i - \bar y)
\end{split}
$$


Окей, осталось доказать, что $\sum_{i=1}^n (y_i - \hat y_i)(\hat y_i - \overline Y) = 0$, и все будет найс.

Так как $b_0 = \overline Y - b_1 \overline X$, а $b_1 = \dfrac{\sum_{i=1}^n (x_i - \bar x)(y_i - \bar y)}{\sum_{i=1}^n (x_i - \bar x)^2}$ имеем

$$
\begin{split}
\sum_{i=1}^n (y_i - \hat y_i)(\hat y_i - \overline Y) 
&= \sum_{i=1}^n (y_i - b_0 - b_1 x_i) (b_0 + b_1 x_i - \overline Y) = \\
&= \sum_{i=1}^n (y_i - \overline Y + b_1 \overline X - b_1x_i) (\overline Y - b_1 \overline X + b_1 x_i - \overline Y) = \\
&= \sum_{i=1}^n \big( (y_i - \overline Y) - b_1 (x_i - \overline X) \big) \cdot b_1 (x_i - \overline X) = \\ 
&= \sum_{i=1}^n \big( b_1 (x_i - \overline X) (y_i - \overline Y) - b_1^2 (x_i - \overline X)^2 \big) = \\
&= \sum_{i=1}^n b_1 (x_i - \overline X) (y_i - \overline Y) - \sum_{i=1}^n b_1^2 (x_i - \overline X)^2 = \\
&= b_1 \sum_{i=1}^n (x_i - \overline X) (y_i - \overline Y) - b_1^2 \sum_{i=1}^n (x_i - \overline X) = \\
&= \frac{\Big( \sum_{i=1}^n (x_i - \overline X) (y_i - \overline Y) \Big)^2}{\sum_{i=1}^n (x_i - \overline X)^2} - \frac{\Big( \sum_{i=1}^n (x_i - \overline X) (y_i - \overline Y) \Big)^2 \cdot \cancel{ \sum_{i=1}^n (x_i - \overline X)^2} }{\Big( \sum_{i=1}^n (x_i - \overline X)^2\Big)^\cancel{2}} = \\
& = \frac{\Big( \sum_{i=1}^n (x_i - \overline X) (y_i - \overline Y) \Big)^2}{\sum_{i=1}^n (x_i - \overline X)^2} - \frac{\Big( \sum_{i=1}^n (x_i - \overline X) (y_i - \overline Y) \Big)^2}{\sum_{i=1}^n (x_i - \overline X)^2} = 0 \qed
\end{split}
$$
:::

:::{.remark}
При доказательстве утверждения мы использовали интерсепт $b_0$. В принципе, нам никто не запрещает построить линейную регрессию без интерсепта --- вида $y_i = b_1 x_i + e_i$. Однако для такого случая соотношение [-@prp-tss] выполняться не будет. Оно справедливо только для моделей с интерсептом.

Работая с регрессией --- как простой, так и сложной --- мы всегда будем иметь дело с моделями, содержащими интерсепт, поэтому утверждение [-@prp-tss] всегда будет верным.
:::


***

###### Session Info {#session_info .unnumbered}

```{r session-info}
sessionInfo()
```

```{=html}
<script type="text/javascript" src="./js/chapter.js"></script>
```
